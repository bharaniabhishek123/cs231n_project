{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be using pytorch datasets and data loaders to implement  residual U-net model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset, sampler, Subset, WeightedRandomSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import skimage \n",
    "from skimage import io \n",
    "import logging\n",
    "from PIL import Image\n",
    "\n",
    "import os \n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, ToPILImage\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000  # incase PIL gives error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/abharani/cs231n_project'\n",
    "data_dir = '/home/abharani/cs231n_project/data'\n",
    "dir_checkpoint = os.path.join(root_dir, 'checkpoints')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset I \n",
    "Created from Image Folder and performing save operation for every image it pre-processes. \n",
    "\n",
    "In one epoch, it will save all the images and then we can use the saved images later to actual training. This will save pre-processing image everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import datasets\n",
    "\n",
    "\n",
    "# class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "#     \"\"\"Custom dataset that includes image file paths. Extends\n",
    "#     torchvision.datasets.ImageFolder\n",
    "#     \"\"\"\n",
    "       \n",
    "#     @classmethod\n",
    "#     def preprocess(cls, image):\n",
    "#         WINDOW_SIZE = 128\n",
    "#         STRIDE = 64\n",
    "#         K = 16\n",
    "        \n",
    "#         image, best_coordinates, best_regions = generate_patches(image, window_size=WINDOW_SIZE, stride=STRIDE, k=K)\n",
    "#         glued_image = glue_to_one_picture(best_regions, window_size=WINDOW_SIZE, k=K)\n",
    "        \n",
    "#         return glued_image           \n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         # this is what ImageFolder normally returns \n",
    "#         original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "\n",
    "#         image, isup_grade = np.array(original_tuple[0]), original_tuple[1]\n",
    "#         image = self.preprocess(image)\n",
    "#         PIL_image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "#         # Let's get the image file path and extract file_name\n",
    "#         path = self.imgs[index][0]\n",
    "#         file_name = path.split(\"/\")[-1].replace(\".jpg\", \"_g.jpg\")\n",
    "#         PIL_image.save(file_name)\n",
    "        \n",
    "#         # Apply Tranformation\n",
    "#         transform=Compose([Resize((224,224)),ToTensor(), \n",
    "#                            Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "#         image = transform(PIL_image)\n",
    "\n",
    "#         return {'image': image, 'isup_grade' : isup_grade}  \n",
    "\n",
    "    \n",
    "# # EXAMPLE USAGE:\n",
    "# # instantiate the dataset and dataloader\n",
    "\n",
    "# # dataset_trial = ImageFolderWithPaths('/home/abharani/data/train_images') # our custom dataset\n",
    "# # dataloader_trial = DataLoader(dataset_trial)\n",
    "\n",
    "# # # iterate over data\n",
    "# # for inputs, labels, paths in dataloader_trial:\n",
    "# #     # use the above variables freely\n",
    "# #     print(inputs['image'].shape, labels, paths)\n",
    "# #     break \n",
    "\n",
    "# # sample = dataset_trial[0]\n",
    "# # print(sample.keys())\n",
    "# # print(sample['image'].shape, sample['isup_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset II\n",
    "Created from ImageFolder on big size .tiff files. This dataset class does not include save operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class BiopsyDataClass(Dataset):\n",
    "    \n",
    "#     def __init__(self, image_path, transform=None):\n",
    "#         super(BiopsyDataClass, self).__init__()\n",
    "#         self.data = datasets.ImageFolder(image_path)    # Create data from folder\n",
    "        \n",
    "#         self.transform = transform\n",
    "        \n",
    "#     @classmethod\n",
    "#     def preprocess(cls, image):\n",
    "#         WINDOW_SIZE = 128\n",
    "#         STRIDE = 64\n",
    "#         K = 16\n",
    "        \n",
    "#         image, best_coordinates, best_regions = generate_patches(image, window_size=WINDOW_SIZE, stride=STRIDE, k=K)\n",
    "#         glued_image = glue_to_one_picture(best_regions, window_size=WINDOW_SIZE, k=K)\n",
    "        \n",
    "#         return glued_image        \n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "\n",
    "#         image, isup_grade = self.data[idx]\n",
    "#         image = np.array(image)\n",
    "#         image = self.preprocess(image)\n",
    "#         PIL_image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "        \n",
    "#         if self.transform:\n",
    "#             image = self.transform(PIL_image) \n",
    "# #             Image.save()\n",
    "# #             print(PIL_image.type)\n",
    "#         return {'image': image, 'isup_grade' : isup_grade}        \n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset III\n",
    "Created from csv and then do a lookup for image_id inside train_images folder. Before using this filter the csv to exclude the image_id not present in train_images folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BasicDataset(Dataset):\n",
    "#     def __init__(self, csv_file, data_dir, transform=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             csv_file (string): Path to the csv file with annotations.\n",
    "#             root_dir (string): Directory with all the images.\n",
    "#             transform (callable, optional): Optional transform to be applied\n",
    "#                 on a sample.\n",
    "#         \"\"\"        \n",
    "#         self.data_frame = pd.read_csv(csv_file)\n",
    "\n",
    "#         ## Local images only  \n",
    "        \n",
    "#         # local_files_df = DataFrame([os.path.splitext(filename)[0] for filename in os.listdir('/Users/abharani/Documents/myworkspace/cs231n_project/data/train_images/') if filename.endswith(\".tiff\")], columns=[\"image_id\"])\n",
    "#         # self.data_frame = local_files_df.join(data_frame.set_index('image_id'), on='image_id')\n",
    "#         # ##\n",
    "\n",
    "#         self.data_dir = data_dir\n",
    "#         self.transform = transform\n",
    "#         logging.info('Creating dataset with {} examples'.format(len(self.data_frame)))\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data_frame)\n",
    "\n",
    "#     @classmethod\n",
    "#     def preprocess(cls, image):\n",
    "#         WINDOW_SIZE = 128\n",
    "#         STRIDE = 64\n",
    "#         K = 16\n",
    "        \n",
    "#         image, best_coordinates, best_regions = generate_patches(image, window_size=WINDOW_SIZE, stride=STRIDE, k=K)\n",
    "#         glued_image = glue_to_one_picture(best_regions, window_size=WINDOW_SIZE, k=K)\n",
    "        \n",
    "#         return glued_image\n",
    "\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         image_id = self.data_frame.iloc[idx]['image_id']\n",
    "#         image_file_path = os.path.join(os.path.join(self.data_dir,'train_images'),image_id + \".tiff\")\n",
    "#         image = skimage.io.MultiImage(image_file_path)[-1]\n",
    "#         image = np.array(image)\n",
    "\n",
    "#         isup_grade = self.data_frame.iloc[idx]['isup_grade']\n",
    "\n",
    "#         image = self.preprocess(image)\n",
    "#         PIL_image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "# #         print(\"Image type {}\".format(image.type)) #numpy.ndarray\n",
    "# #         print(PIL_image.type)                     # Image object      \n",
    "\n",
    "        \n",
    "#         if self.transform:\n",
    "            \n",
    "#             image = self.transform(PIL_image)\n",
    "        \n",
    "#         return {'image': image, 'isup_grade' : isup_grade}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 : \n",
    "Afrer saving the images generated from Dataset class ImageFolderWithPaths. Create dataset using the ImageFolder.\n",
    "\n",
    "\n",
    "Since Data is imbalanced, performing Weighted Random Sampling not Random Sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.25, generate_small=False):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets_created = {}\n",
    "    if generate_small:\n",
    "        print(\"Generating Small Train, Test Dataset\")\n",
    "        datasets_created['train'] = Subset(dataset, train_idx[0:200])\n",
    "        datasets_created['test'] = Subset(dataset, val_idx[0:50])\n",
    "    else:\n",
    "        datasets_created['train'] = Subset(dataset, train_idx)\n",
    "        datasets_created['test'] = Subset(dataset, val_idx)        \n",
    "    return datasets_created\n",
    "\n",
    "dataset = ImageFolder('/home/abharani/cs231n_project/glued_images/data', \n",
    "                      transform=Compose([Resize((224,224)),ToTensor(), \n",
    "                                        Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                   std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "# print(len(dataset))\n",
    "# datasets = train_val_dataset(dataset,generate_small=False)\n",
    "# print(len(datasets['train']))\n",
    "# print(len(datasets['val']))\n",
    "# # The original dataset is available in the Subset class\n",
    "# print(datasets['train'].dataset)\n",
    "\n",
    "\n",
    "# x,y = datasets['train'][0]\n",
    "# print(x.shape, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Train, Valid and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Small Train, Test Dataset\n",
      "Generating Small Train, Test Dataset\n",
      "Dataset set size 10615\n",
      "Train set size 150\n",
      "Validation set size 50\n",
      "Test set size 50\n"
     ]
    }
   ],
   "source": [
    "dataset_final = {}\n",
    "datasets_created_trial_1 = train_val_dataset(dataset,val_split=0.10,generate_small=True)\n",
    "datasets_created_trial_2 = train_val_dataset(datasets_created_trial_1['train'],val_split=0.25,generate_small=True)\n",
    "\n",
    "dataset_final['test'] = datasets_created_trial_1['test']\n",
    "dataset_final['train'] = datasets_created_trial_2['train']\n",
    "dataset_final['val'] = datasets_created_trial_2['test']\n",
    "\n",
    "print(\"Dataset set size {}\".format(len(dataset)))\n",
    "print(\"Train set size {}\".format(len(dataset_final['train'])))\n",
    "print(\"Validation set size {}\".format(len(dataset_final['val'])))\n",
    "print(\"Test set size {}\".format(len(dataset_final['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(datasets['train'])):\n",
    "#     sample = dataset[i]\n",
    "\n",
    "#     print(i, sample['image'].shape, sample['isup_grade'])\n",
    "\n",
    "#     if i == 3:\n",
    "#         plt.show()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing WeightedRandomSampling using link : https://towardsdatascience.com/pytorch-basics-sampling-samplers-2a0f29f0bf2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate class weights and target_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes: \n",
      " defaultdict(<class 'int'>, {0: 48, 5: 13, 3: 21, 4: 15, 1: 40, 2: 13})\n",
      "Class weights : \n",
      " tensor([0.0208, 0.0769, 0.0476, 0.0667, 0.0250, 0.0769])\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "count_dict = defaultdict(int)\n",
    "target_list = []\n",
    "\n",
    "#Generate target_list of all labels and count dict of all classes\n",
    "\n",
    "for i, (image, label) in enumerate(dataset_final['train']):    \n",
    "    count_dict[label] += 1\n",
    "    target_list.append(label)\n",
    "    \n",
    "#     print(i, image.shape,sample)    \n",
    "#     if i== 3:\n",
    "#         break\n",
    "\n",
    "\n",
    "count_dict\n",
    "print(\"Distribution of classes: \\n\", count_dict)\n",
    "\n",
    "class_count = [i for i in count_dict.values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "print(\"Class weights : \\n\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = torch.tensor(target_list)\n",
    "target_list = target_list[torch.randperm(len(target_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0769, 0.0769, 0.0208, 0.0250, 0.0208, 0.0208, 0.0476, 0.0769, 0.0667, 0.0769, 0.0667, 0.0208, 0.0769, 0.0250, 0.0208, 0.0208, 0.0667, 0.0667, 0.0208, 0.0769, 0.0208, 0.0769, 0.0667, 0.0667,\n",
       "        0.0769, 0.0208, 0.0208, 0.0769, 0.0769, 0.0208, 0.0208, 0.0667, 0.0250, 0.0769, 0.0769, 0.0667, 0.0208, 0.0250, 0.0769, 0.0208, 0.0769, 0.0208, 0.0769, 0.0769, 0.0476, 0.0476, 0.0769, 0.0769,\n",
       "        0.0667, 0.0208, 0.0250, 0.0769, 0.0208, 0.0667, 0.0208, 0.0208, 0.0769, 0.0208, 0.0250, 0.0208, 0.0667, 0.0769, 0.0250, 0.0208, 0.0208, 0.0769, 0.0208, 0.0769, 0.0208, 0.0769, 0.0208, 0.0250,\n",
       "        0.0769, 0.0476, 0.0769, 0.0208, 0.0208, 0.0667, 0.0208, 0.0208, 0.0667, 0.0667, 0.0208, 0.0208, 0.0476, 0.0476, 0.0208, 0.0769, 0.0667, 0.0769, 0.0769, 0.0769, 0.0250, 0.0476, 0.0769, 0.0769,\n",
       "        0.0250, 0.0250, 0.0769, 0.0476, 0.0667, 0.0667, 0.0208, 0.0769, 0.0476, 0.0208, 0.0208, 0.0250, 0.0769, 0.0769, 0.0476, 0.0250, 0.0208, 0.0208, 0.0208, 0.0476, 0.0769, 0.0208, 0.0208, 0.0769,\n",
       "        0.0208, 0.0667, 0.0667, 0.0250, 0.0208, 0.0208, 0.0769, 0.0769, 0.0769, 0.0769, 0.0208, 0.0667, 0.0208, 0.0769, 0.0667, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0250, 0.0208, 0.0476,\n",
       "        0.0769, 0.0208, 0.0769, 0.0476, 0.0769, 0.0769])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights_all = class_weights[target_list]\n",
    "class_weights_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use gpu and float as dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "\n",
    "base_model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# list(base_model.children())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "        self.base_layers = list(base_model.children())                \n",
    "        \n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)  \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        # MY experiment\n",
    "        self.fc = nn.Linear(301056, 6)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(input)            \n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "\n",
    "        # MY experiment\n",
    "        x = out.view(out.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # x = F.adaptive_avg_pool2d(out, output_size=1)\n",
    "        # print(x.size(),x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use new_train_enhanced_images below for training on images glued ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_8x(model, val_loader, device):\n",
    "    \"\"\"Evaluation model on validation set\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    n_val = len(val_loader)  # the number of batch\n",
    "    total_loss = 0\n",
    "    total_correct = 0.00\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batch in val_loader:\n",
    "        imgs, true_label = batch[0], batch[1]\n",
    "        imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "        true_label = true_label.to(device=device, dtype=torch.long)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(imgs)\n",
    "            _, pred_label = torch.max(logits, 1)\n",
    "\n",
    "        total_loss += criterion(logits, true_label)\n",
    "        correct = torch.sum(pred_label == true_label.data)\n",
    "        total_correct += correct\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    print(\"total_correct\", total_correct)\n",
    "    return total_loss / n_val , total_correct/n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_train_enhanced_images(model, device, epochs=5, batch_size=32, lr=0.001, val_percent=0.1, save_cp=True):\n",
    "    \n",
    "    train_loader = DataLoader(dataset_final['train'],batch_size=32, sampler = weighted_sampler, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(dataset_final['val'],batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    dataset_sizes = {x: len(dataset_final[x]) for x in dataset_final.keys()}\n",
    "\n",
    "    # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "    writer = SummaryWriter('runs/experiment_3_gluedimages')\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0.00 \n",
    "        epoch_accuracy = 0.00\n",
    "        running_loss = 0.00\n",
    "        running_corrects = 0.00 \n",
    "        \n",
    "        for i, batch in enumerate(train_loader) :\n",
    "            \n",
    "            imgs = batch[0].to(device, dtype=torch.float32)\n",
    "            true_label = batch[1].to(device=device, dtype=torch.long)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(imgs)\n",
    "            _, pred_label = torch.max(logits, 1)\n",
    "            \n",
    "            loss = criterion(logits, true_label)\n",
    "            correct = torch.sum(pred_label == true_label.data)\n",
    "            train_acc = 100.00 * correct / imgs.size(0)\n",
    "           \n",
    "            epoch_loss += loss.item()\n",
    "            running_loss += loss.item() * imgs.size(0) # batch_size \n",
    "            running_corrects += correct\n",
    "\n",
    "            #tensorboard\n",
    "#             img_grid = torchvision.utils.make_grid(imgs)\n",
    "            writer.add_scalar('Training Loss', loss.item(), global_step)\n",
    "            writer.add_scalar('Training Accuracy', train_acc, global_step)\n",
    "\n",
    "            if i %10 ==0: # print loss every 10th step\n",
    "                print('Epoch {} , Train Step {}, Training loss: {}, Train Accuracy: {}'.\n",
    "                      format(epoch, i , loss.item(), train_acc))\n",
    "                \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_value_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "\n",
    "            global_step += 1\n",
    "#             if global_step % (len(dataset) // (10 * batch_size)) == 0:\n",
    "                # for tag, value in model.named_parameters():\n",
    "                #     tag = tag.replace('.', '/')\n",
    "                #     writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                #     writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "                \n",
    "            \n",
    "            if global_step % 1 ==0:\n",
    "                val_loss, val_acc = eval_model_8x(model, val_loader, device)\n",
    "                scheduler.step(val_loss)\n",
    "                print('Validation loss: {}, Validation Accuracy : {}'.format(val_loss, val_acc))\n",
    "\n",
    "                #tensorboard\n",
    "#                 writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "                writer.add_scalar('Validation Loss', val_loss, global_step)\n",
    "                writer.add_scalar('Validation Accuracy', val_acc, global_step)\n",
    "                writer.add_images('images', imgs, global_step)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes['train']\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes['train']\n",
    "        print('Epoch {} Summary, Loss: {:.4f} Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "#         if save_cp:\n",
    "#             try:\n",
    "#                 os.mkdir(dir_checkpoint)\n",
    "#                 print('Created checkpoint directory')\n",
    "#             except OSError:\n",
    "#                 pass\n",
    "#             torch.save(model.state_dict(),\n",
    "#                        dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n",
    "#             print(f'Checkpoint {epoch + 1} saved !')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with 8.x image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 0/9\n",
      "Epoch 0 , Train Step 0, Training loss: 1.7955344915390015, Train Accuracy: 25.0\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.797201156616211, Validation Accuracy : 7.0\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.7959904670715332, Validation Accuracy : 7.0\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.7885122299194336, Validation Accuracy : 7.0\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.855508804321289, Validation Accuracy : 7.0\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.9323768615722656, Validation Accuracy : 7.0\n",
      "Epoch 0 Summary, Loss: 1.7383 Acc: 0.2733\n",
      "Epoch 1/9\n",
      "Epoch 1 , Train Step 0, Training loss: 1.8381494283676147, Train Accuracy: 31.25\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.9581928253173828, Validation Accuracy : 7.0\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.943925142288208, Validation Accuracy : 7.0\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.907407522201538, Validation Accuracy : 7.0\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.8820734024047852, Validation Accuracy : 7.0\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.8277971744537354, Validation Accuracy : 7.0\n",
      "Epoch 1 Summary, Loss: 1.7947 Acc: 0.3400\n",
      "Epoch 2/9\n",
      "Epoch 2 , Train Step 0, Training loss: 1.8323442935943604, Train Accuracy: 18.75\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.799685001373291, Validation Accuracy : 7.0\n",
      "total_correct tensor(11., device='cuda:0')\n",
      "Validation loss: 1.8038078546524048, Validation Accuracy : 5.5\n",
      "total_correct tensor(11., device='cuda:0')\n",
      "Validation loss: 1.7798473834991455, Validation Accuracy : 5.5\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.7402117252349854, Validation Accuracy : 7.0\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.7151423692703247, Validation Accuracy : 7.0\n",
      "Epoch 2 Summary, Loss: 1.7738 Acc: 0.1933\n",
      "Epoch 3/9\n",
      "Epoch 3 , Train Step 0, Training loss: 1.6881537437438965, Train Accuracy: 25.0\n",
      "total_correct tensor(15., device='cuda:0')\n",
      "Validation loss: 1.7078254222869873, Validation Accuracy : 7.5\n",
      "total_correct tensor(18., device='cuda:0')\n",
      "Validation loss: 1.7267069816589355, Validation Accuracy : 9.0\n",
      "total_correct tensor(15., device='cuda:0')\n",
      "Validation loss: 1.7506412267684937, Validation Accuracy : 7.5\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.7764551639556885, Validation Accuracy : 7.0\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.7850384712219238, Validation Accuracy : 7.0\n",
      "Epoch 3 Summary, Loss: 1.7116 Acc: 0.2333\n",
      "Epoch 4/9\n",
      "Epoch 4 , Train Step 0, Training loss: 1.8247519731521606, Train Accuracy: 21.875\n",
      "total_correct tensor(14., device='cuda:0')\n",
      "Validation loss: 1.7727843523025513, Validation Accuracy : 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/cs231n/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/envs/cs231n/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/envs/cs231n/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/envs/cs231n/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/cs231n/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/envs/cs231n/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/envs/cs231n/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/envs/cs231n/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-9f60a566abb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_train_enhanced_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_percent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_cp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-003a244bcf0d>\u001b[0m in \u001b[0;36mnew_train_enhanced_images\u001b[0;34m(model, device, epochs, batch_size, lr, val_percent, save_cp)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model_8x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation loss: {}, Validation Accuracy : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-b2e433417ef4>\u001b[0m in \u001b[0;36meval_model_8x\u001b[0;34m(model, val_loader, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device='cpu'\n",
    "num_class = 6\n",
    "\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "# Comment out to finetune further\n",
    "for l in model.base_layers:\n",
    "    for param in l.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)        \n",
    "        \n",
    "# model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=1)\n",
    "\n",
    "model = new_train_enhanced_images(model, device, epochs=10, batch_size=32, lr=0.001,val_percent=0.1, save_cp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs231n]",
   "language": "python",
   "name": "conda-env-cs231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
