{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be using pytorch datasets and data loaders to implement  residual U-net model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset, sampler, Subset, WeightedRandomSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import skimage \n",
    "from skimage import io \n",
    "import logging\n",
    "from PIL import Image\n",
    "\n",
    "import os \n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, ToPILImage\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000  # incase PIL gives error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/abharani/cs231n_project'\n",
    "data_dir = '/home/abharani/cs231n_project/data'\n",
    "dir_checkpoint = os.path.join(root_dir, 'checkpoints')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset I \n",
    "Created from Image Folder and performing save operation for every image it pre-processes. \n",
    "\n",
    "In one epoch, it will save all the images and then we can use the saved images later to actual training. This will save pre-processing image everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import datasets\n",
    "\n",
    "\n",
    "# class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "#     \"\"\"Custom dataset that includes image file paths. Extends\n",
    "#     torchvision.datasets.ImageFolder\n",
    "#     \"\"\"\n",
    "       \n",
    "#     @classmethod\n",
    "#     def preprocess(cls, image):\n",
    "#         WINDOW_SIZE = 128\n",
    "#         STRIDE = 64\n",
    "#         K = 16\n",
    "        \n",
    "#         image, best_coordinates, best_regions = generate_patches(image, window_size=WINDOW_SIZE, stride=STRIDE, k=K)\n",
    "#         glued_image = glue_to_one_picture(best_regions, window_size=WINDOW_SIZE, k=K)\n",
    "        \n",
    "#         return glued_image           \n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         # this is what ImageFolder normally returns \n",
    "#         original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "\n",
    "#         image, isup_grade = np.array(original_tuple[0]), original_tuple[1]\n",
    "#         image = self.preprocess(image)\n",
    "#         PIL_image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "#         # Let's get the image file path and extract file_name\n",
    "#         path = self.imgs[index][0]\n",
    "#         file_name = path.split(\"/\")[-1].replace(\".jpg\", \"_g.jpg\")\n",
    "#         PIL_image.save(file_name)\n",
    "        \n",
    "#         # Apply Tranformation\n",
    "#         transform=Compose([Resize((224,224)),ToTensor(), \n",
    "#                            Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "#         image = transform(PIL_image)\n",
    "\n",
    "#         return {'image': image, 'isup_grade' : isup_grade}  \n",
    "\n",
    "    \n",
    "# # EXAMPLE USAGE:\n",
    "# # instantiate the dataset and dataloader\n",
    "\n",
    "# # dataset_trial = ImageFolderWithPaths('/home/abharani/data/train_images') # our custom dataset\n",
    "# # dataloader_trial = DataLoader(dataset_trial)\n",
    "\n",
    "# # # iterate over data\n",
    "# # for inputs, labels, paths in dataloader_trial:\n",
    "# #     # use the above variables freely\n",
    "# #     print(inputs['image'].shape, labels, paths)\n",
    "# #     break \n",
    "\n",
    "# # sample = dataset_trial[0]\n",
    "# # print(sample.keys())\n",
    "# # print(sample['image'].shape, sample['isup_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset II\n",
    "Created from ImageFolder on big size .tiff files. This dataset class does not include save operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class BiopsyDataClass(Dataset):\n",
    "    \n",
    "#     def __init__(self, image_path, transform=None):\n",
    "#         super(BiopsyDataClass, self).__init__()\n",
    "#         self.data = datasets.ImageFolder(image_path)    # Create data from folder\n",
    "        \n",
    "#         self.transform = transform\n",
    "        \n",
    "#     @classmethod\n",
    "#     def preprocess(cls, image):\n",
    "#         WINDOW_SIZE = 128\n",
    "#         STRIDE = 64\n",
    "#         K = 16\n",
    "        \n",
    "#         image, best_coordinates, best_regions = generate_patches(image, window_size=WINDOW_SIZE, stride=STRIDE, k=K)\n",
    "#         glued_image = glue_to_one_picture(best_regions, window_size=WINDOW_SIZE, k=K)\n",
    "        \n",
    "#         return glued_image        \n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "\n",
    "#         image, isup_grade = self.data[idx]\n",
    "#         image = np.array(image)\n",
    "#         image = self.preprocess(image)\n",
    "#         PIL_image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "        \n",
    "#         if self.transform:\n",
    "#             image = self.transform(PIL_image) \n",
    "# #             Image.save()\n",
    "# #             print(PIL_image.type)\n",
    "#         return {'image': image, 'isup_grade' : isup_grade}        \n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset III\n",
    "Created from csv and then do a lookup for image_id inside train_images folder. Before using this filter the csv to exclude the image_id not present in train_images folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BasicDataset(Dataset):\n",
    "#     def __init__(self, csv_file, data_dir, transform=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             csv_file (string): Path to the csv file with annotations.\n",
    "#             root_dir (string): Directory with all the images.\n",
    "#             transform (callable, optional): Optional transform to be applied\n",
    "#                 on a sample.\n",
    "#         \"\"\"        \n",
    "#         self.data_frame = pd.read_csv(csv_file)\n",
    "\n",
    "#         ## Local images only  \n",
    "        \n",
    "#         # local_files_df = DataFrame([os.path.splitext(filename)[0] for filename in os.listdir('/Users/abharani/Documents/myworkspace/cs231n_project/data/train_images/') if filename.endswith(\".tiff\")], columns=[\"image_id\"])\n",
    "#         # self.data_frame = local_files_df.join(data_frame.set_index('image_id'), on='image_id')\n",
    "#         # ##\n",
    "\n",
    "#         self.data_dir = data_dir\n",
    "#         self.transform = transform\n",
    "#         logging.info('Creating dataset with {} examples'.format(len(self.data_frame)))\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data_frame)\n",
    "\n",
    "#     @classmethod\n",
    "#     def preprocess(cls, image):\n",
    "#         WINDOW_SIZE = 128\n",
    "#         STRIDE = 64\n",
    "#         K = 16\n",
    "        \n",
    "#         image, best_coordinates, best_regions = generate_patches(image, window_size=WINDOW_SIZE, stride=STRIDE, k=K)\n",
    "#         glued_image = glue_to_one_picture(best_regions, window_size=WINDOW_SIZE, k=K)\n",
    "        \n",
    "#         return glued_image\n",
    "\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         image_id = self.data_frame.iloc[idx]['image_id']\n",
    "#         image_file_path = os.path.join(os.path.join(self.data_dir,'train_images'),image_id + \".tiff\")\n",
    "#         image = skimage.io.MultiImage(image_file_path)[-1]\n",
    "#         image = np.array(image)\n",
    "\n",
    "#         isup_grade = self.data_frame.iloc[idx]['isup_grade']\n",
    "\n",
    "#         image = self.preprocess(image)\n",
    "#         PIL_image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "# #         print(\"Image type {}\".format(image.type)) #numpy.ndarray\n",
    "# #         print(PIL_image.type)                     # Image object      \n",
    "\n",
    "        \n",
    "#         if self.transform:\n",
    "            \n",
    "#             image = self.transform(PIL_image)\n",
    "        \n",
    "#         return {'image': image, 'isup_grade' : isup_grade}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 : \n",
    "Afrer saving the images generated from Dataset class ImageFolderWithPaths. Create dataset using the ImageFolder.\n",
    "\n",
    "\n",
    "Since Data is imbalanced, performing Weighted Random Sampling not Random Sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.25, generate_small=False):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets_created = {}\n",
    "    if generate_small:\n",
    "        print(\"Generating Small Train, Test Dataset\")\n",
    "        datasets_created['train'] = Subset(dataset, train_idx[0:200])\n",
    "        datasets_created['test'] = Subset(dataset, val_idx[0:50])\n",
    "    else:\n",
    "        datasets_created['train'] = Subset(dataset, train_idx)\n",
    "        datasets_created['test'] = Subset(dataset, val_idx)        \n",
    "    return datasets_created\n",
    "\n",
    "dataset = ImageFolder('/home/abharani/cs231n_project/glued_images/data', \n",
    "                      transform=Compose([Resize((224,224)),ToTensor(), \n",
    "                                        Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                   std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "# print(len(dataset))\n",
    "# datasets = train_val_dataset(dataset,generate_small=False)\n",
    "# print(len(datasets['train']))\n",
    "# print(len(datasets['val']))\n",
    "# # The original dataset is available in the Subset class\n",
    "# print(datasets['train'].dataset)\n",
    "\n",
    "\n",
    "# x,y = datasets['train'][0]\n",
    "# print(x.shape, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Train, Valid and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset set size 10615\n",
      "Train set size 7164\n",
      "Validation set size 2389\n",
      "Test set size 1062\n"
     ]
    }
   ],
   "source": [
    "dataset_final = {}\n",
    "datasets_created_trial_1 = train_val_dataset(dataset,val_split=0.10,generate_small=False)\n",
    "datasets_created_trial_2 = train_val_dataset(datasets_created_trial_1['train'],val_split=0.25,generate_small=False)\n",
    "\n",
    "dataset_final['test'] = datasets_created_trial_1['test']\n",
    "dataset_final['train'] = datasets_created_trial_2['train']\n",
    "dataset_final['val'] = datasets_created_trial_2['test']\n",
    "\n",
    "print(\"Dataset set size {}\".format(len(dataset)))\n",
    "print(\"Train set size {}\".format(len(dataset_final['train'])))\n",
    "print(\"Validation set size {}\".format(len(dataset_final['val'])))\n",
    "print(\"Test set size {}\".format(len(dataset_final['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(datasets['train'])):\n",
    "#     sample = dataset[i]\n",
    "\n",
    "#     print(i, sample['image'].shape, sample['isup_grade'])\n",
    "\n",
    "#     if i == 3:\n",
    "#         plt.show()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing WeightedRandomSampling using link : https://towardsdatascience.com/pytorch-basics-sampling-samplers-2a0f29f0bf2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate class weights and target_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes: \n",
      " defaultdict(<class 'int'>, {0: 1955, 4: 834, 1: 1798, 2: 915, 5: 823, 3: 839})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0005, 0.0012, 0.0006, 0.0011, 0.0012, 0.0012])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "count_dict = defaultdict(int)\n",
    "target_list = []\n",
    "\n",
    "#Generate target_list of all labels and count dict of all classes\n",
    "\n",
    "for i, (image, label) in enumerate(dataset_final['train']):    \n",
    "    count_dict[label] += 1\n",
    "    target_list.append(label)\n",
    "    \n",
    "#     print(i, image.shape,sample)    \n",
    "#     if i== 3:\n",
    "#         break\n",
    "\n",
    "\n",
    "count_dict\n",
    "print(\"Distribution of classes: \\n\", count_dict)\n",
    "\n",
    "class_count = [i for i in count_dict.values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "print(\"Class weights : \\n\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = torch.tensor(target_list)\n",
    "target_list = target_list[torch.randperm(len(target_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0012, 0.0012, 0.0005,  ..., 0.0005, 0.0012, 0.0012])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights_all = class_weights[target_list]\n",
    "class_weights_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use gpu and float as dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "\n",
    "base_model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# list(base_model.children())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "        self.base_layers = list(base_model.children())                \n",
    "        \n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)  \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        # MY experiment\n",
    "        self.fc = nn.Linear(301056, 6)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(input)            \n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "\n",
    "        # MY experiment\n",
    "        x = out.view(out.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # x = F.adaptive_avg_pool2d(out, output_size=1)\n",
    "        # print(x.size(),x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use new_train_enhanced_images below for training on images glued ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_8x(model, val_loader, device):\n",
    "    \"\"\"Evaluation without the densecrf with the dice coefficient\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    n_val = len(val_loader)  # the number of batch\n",
    "    tot = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        imgs, true_label = batch[0], batch[1]\n",
    "        imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "        true_label = true_label.to(device=device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            pred_label = model(imgs)\n",
    "\n",
    "        tot += F.cross_entropy(pred_label, true_label).item()\n",
    "\n",
    "    model.train()\n",
    "    return tot / n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_train_enhanced_images(model, device, epochs=5, batch_size=32, lr=0.001, val_percent=0.1, save_cp=True):\n",
    "    \n",
    "#     dataloaders = {x:DataLoader(datasets[x],batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train','val']}\n",
    "    train_loader = DataLoader(dataset_final['train'],batch_size=32, sampler = weighted_sampler, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(dataset_final['val'],batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "#     x,y = next(iter(dataloaders['train']))\n",
    "#     print(x.shape, y.shape)\n",
    "\n",
    "    dataset_sizes = {x: len(dataset_final[x]) for x in dataset_final.keys()}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # dataset = BasicDataset(csv_file=tiny_csv_path, data_dir=data_dir)\n",
    "    # n_val = int(len(dataset) * val_percent)\n",
    "    # n_train = len(dataset) - n_val\n",
    "    # train, val = random_split(dataset, [n_train, n_val])\n",
    "    # train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    # val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
    "    # dataloaders = {'train': train_loader, 'val':val_loader}\n",
    "    # dataset_sizes = {'train':n_train, 'val':n_val}\n",
    "\n",
    "    # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "    writer = SummaryWriter('runs/experiment_3_gluedimages')\n",
    "    global_step = 0\n",
    "\n",
    "    # print(f'''Starting training:\n",
    "    #     Epochs:          {epochs}\n",
    "    #     Batch size:      {batch_size}\n",
    "    #     Learning rate:   {lr}\n",
    "    #     Training size:   {n_train}\n",
    "    #     Validation size: {n_val}\n",
    "    #     Checkpoints:     {save_cp}\n",
    "    #     Device:          {device.type}\n",
    "    # ''')\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0.00 \n",
    "        epoch_accuracy = 0.00\n",
    "        running_loss = 0.00\n",
    "        running_corrects = 0.00 \n",
    "        for i, batch in enumerate(train_loader) :\n",
    "            \n",
    "#             print('Epoch {}, Step{}'.format(epoch, i))\n",
    "            \n",
    "            imgs = batch[0].to(device, dtype=torch.float32)\n",
    "        \n",
    "#             print(imgs.size())\n",
    "#             imgs = torch.reshape(imgs,(imgs.shape[0],imgs.shape[3],imgs.shape[1],imgs.shape[2])) #  inputs.reshape [N, C, W, H]\n",
    "\n",
    "            true_label = batch[1].to(device=device, dtype=torch.long)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            _, pred_label = torch.max(logits, 1)\n",
    "            loss = criterion(logits, true_label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            running_loss += loss.item() * imgs.size(0) # batch_size \n",
    "            running_corrects += torch.sum(pred_label == true_label.data)\n",
    "\n",
    "            #tensorboard\n",
    "            img_grid = torchvision.utils.make_grid(imgs)\n",
    "            # writer.add_image(\"training Images grid\" , img_grid)\n",
    "            writer.add_scalar('Training Loss', loss.item(), global_step)\n",
    "\n",
    "            if i %10 ==0: # print loss every 10th step\n",
    "                print('Epoch {} , Train Step {}, Training loss: {}'.format(epoch, i , loss.item()))\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_value_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            global_step += 1\n",
    "#             if global_step % (len(dataset) // (10 * batch_size)) == 0:\n",
    "                # for tag, value in model.named_parameters():\n",
    "                #     tag = tag.replace('.', '/')\n",
    "                #     writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                #     writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "        val_score = eval_model_8x(model, val_loader, device)\n",
    "        scheduler.step()\n",
    "        print('Validation loss: {}'.format(val_score))\n",
    "\n",
    "        #tensorboard\n",
    "        writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "        writer.add_scalar('Validation Loss', val_score, global_step)\n",
    "        writer.add_images('images', imgs, global_step)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes['train']\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes['train']\n",
    "        print(' epoch {}, Loss: {:.4f} Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "        writer.add_scalar(\"epoch loss\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"epoch acc\", epoch_acc, epoch)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                print('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(model.state_dict(),\n",
    "                       dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n",
    "            print(f'Checkpoint {epoch + 1} saved !')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with 8.x image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 0/9\n",
      "Epoch 0 , Train Step 0, Training loss: 1.8152673244476318\n",
      "Epoch 0 , Train Step 10, Training loss: 1.7545740604400635\n",
      "Epoch 0 , Train Step 20, Training loss: 1.8226145505905151\n",
      "Epoch 0 , Train Step 30, Training loss: 1.6562457084655762\n",
      "Epoch 0 , Train Step 40, Training loss: 1.7126117944717407\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device='cpu'\n",
    "num_class = 6\n",
    "\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "# Comment out to finetune further\n",
    "for l in model.base_layers:\n",
    "    for param in l.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)        \n",
    "        \n",
    "# model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=1)\n",
    "\n",
    "model = new_train_enhanced_images(model, device, epochs=10, batch_size=32, lr=0.001,val_percent=0.1, save_cp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs231n]",
   "language": "python",
   "name": "conda-env-cs231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
