{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be using pytorch datasets and data loaders to implement u-net residual model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset, sampler,Subset\n",
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n",
    "\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 : Create dataset and dataloader  \n",
    "#### Creating the dataset from Imagefolder having folder structure like\n",
    "--Image-folder\n",
    "\n",
    "    --0 \n",
    "    \n",
    "    --1 \n",
    "    \n",
    "    --2\n",
    "    \n",
    "    .\n",
    "    \n",
    "    .\n",
    "    \n",
    "    --5\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10615\n",
      "7961\n",
      "2654\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 10615\n",
      "    Root location: /home/abharani/data/output\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "               Normalize: (TensorImage,object) -> encodes (TensorImage,object) -> decodes\n",
      "           )\n",
      "torch.Size([3, 224, 224]) 5\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/home/abharani/cs231n_project'\n",
    "data_dir =  '/home/abharani/data'\n",
    "# csv_path = os.path.join(data_dir,'train.csv')\n",
    "# small_csv_path = os.path.join(data_dir,'small_train.csv')\n",
    "# tiny_csv_path = os.path.join(data_dir,'tiny_train.csv')\n",
    "dir_checkpoint = os.path.join(root_dir, 'checkpoints')\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.25, generate_small=False):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    if generate_small:\n",
    "        print(\"Generating Small Train/Valid Dataset\")\n",
    "        datasets['train'] = Subset(dataset, train_idx[0:200])\n",
    "        datasets['val'] = Subset(dataset, val_idx[0:50])\n",
    "    else:\n",
    "        datasets['train'] = Subset(dataset, train_idx)\n",
    "        datasets['val'] = Subset(dataset, val_idx)        \n",
    "    return datasets\n",
    "\n",
    "dataset = ImageFolder(os.path.join(data_dir,'output'), transform=Compose([Resize((224,224)),ToTensor(), \n",
    "                                                                      Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                               std=[0.229, 0.224, 0.225]) \n",
    "                                                                      ]))\n",
    "print(len(dataset))\n",
    "datasets = train_val_dataset(dataset,generate_small=False)\n",
    "print(len(datasets['train']))\n",
    "print(len(datasets['val']))\n",
    "# The original dataset is available in the Subset class\n",
    "print(datasets['train'].dataset)\n",
    "\n",
    "\n",
    "x,y = datasets['train'][0]\n",
    "print(x.shape, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(root_path, batch_size, phase):\n",
    "#     transform_dict = {\n",
    "#         'train': transforms.Compose(\n",
    "#         [transforms.RandomResizedCrop(224),\n",
    "#          transforms.RandomHorizontalFlip(),\n",
    "#          transforms.ToTensor(),\n",
    "#          transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                               std=[0.229, 0.224, 0.225]),\n",
    "#          ]),\n",
    "#         'valid': transforms.Compose(\n",
    "#         [transforms.Resize(224),\n",
    "#          transforms.ToTensor(),\n",
    "#          transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                               std=[0.229, 0.224, 0.225]),\n",
    "#          ])}\n",
    "#     data = datasets.ImageFolder(root=root_path, transform=transform_dict[phase])\n",
    "#     print(len(data))\n",
    "#     data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=4)\n",
    "#     return data_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# list(base_model.children())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "        self.base_layers = list(base_model.children())                \n",
    "        \n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)  \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        # MY experiment\n",
    "        self.fc = nn.Linear(301056, 6)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(input)            \n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "\n",
    "        # MY experiment\n",
    "        x = out.view(out.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # x = F.adaptive_avg_pool2d(out, output_size=1)\n",
    "        # print(x.size(),x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_loader, device):\n",
    "    \"\"\"Evaluation without the densecrf with the dice coefficient\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    n_val = len(val_loader)  # the number of batch\n",
    "    tot = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        imgs, true_label = batch[0], batch[1]\n",
    "        imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "        true_label = true_label.to(device=device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            pred_label = model(imgs)\n",
    "\n",
    "        tot += F.cross_entropy(pred_label, true_label).item()\n",
    "\n",
    "    model.train()\n",
    "    return tot / n_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use new_train_model below for 224x224 compressed images inside output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_train_model(model, device, epochs=5, batch_size=32, lr=0.001, val_percent=0.1, save_cp=True):\n",
    "\n",
    "    dataloaders = {x:DataLoader(datasets[x],batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train','val']}\n",
    "    train_loader = dataloaders['train']\n",
    "    val_loader = dataloaders['val']\n",
    "\n",
    "#     x,y = next(iter(dataloaders['train']))\n",
    "#     print(x.shape, y.shape)\n",
    "\n",
    "    dataset_sizes = {x: len(datasets[x]) for x in datasets.keys()}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # dataset = BasicDataset(csv_file=tiny_csv_path, data_dir=data_dir)\n",
    "    # n_val = int(len(dataset) * val_percent)\n",
    "    # n_train = len(dataset) - n_val\n",
    "    # train, val = random_split(dataset, [n_train, n_val])\n",
    "    # train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    # val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
    "    # dataloaders = {'train': train_loader, 'val':val_loader}\n",
    "    # dataset_sizes = {'train':n_train, 'val':n_val}\n",
    "\n",
    "    # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "    writer = SummaryWriter('runs/experiment_2_perfect')\n",
    "    global_step = 0\n",
    "\n",
    "    # print(f'''Starting training:\n",
    "    #     Epochs:          {epochs}\n",
    "    #     Batch size:      {batch_size}\n",
    "    #     Learning rate:   {lr}\n",
    "    #     Training size:   {n_train}\n",
    "    #     Validation size: {n_val}\n",
    "    #     Checkpoints:     {save_cp}\n",
    "    #     Device:          {device.type}\n",
    "    # ''')\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0.00 \n",
    "        epoch_accuracy = 0.00\n",
    "        running_loss = 0.00\n",
    "        running_corrects = 0.00 \n",
    "        for i, batch in enumerate(train_loader) :\n",
    "            \n",
    "#             print('Epoch {}, Step{}'.format(epoch, i))\n",
    "            \n",
    "            imgs = batch[0].to(device, dtype=torch.float32)\n",
    "            # imgs = torch.reshape(imgs,(imgs.shape[0],imgs.shape[3],imgs.shape[1],imgs.shape[2])) #  inputs.reshape [N, C, W, H]\n",
    "\n",
    "            true_label = batch[1].to(device=device, dtype=torch.long)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            _, pred_label = torch.max(logits, 1)\n",
    "            loss = criterion(logits, true_label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            running_loss += loss.item() * imgs.size(0) # batch_size \n",
    "            running_corrects += torch.sum(pred_label == true_label.data)\n",
    "\n",
    "            #tensorboard\n",
    "            img_grid = torchvision.utils.make_grid(imgs)\n",
    "            # writer.add_image(\"training Images grid\" , img_grid)\n",
    "            writer.add_scalar('Training Loss', loss.item(), global_step)\n",
    "\n",
    "            if i %10 ==0: # print loss every 10th step\n",
    "                print('Epoch {} , Train Step {}, Training loss: {}'.format(epoch, i , loss.item()))\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_value_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            global_step += 1\n",
    "#             if global_step % (len(dataset) // (10 * batch_size)) == 0:\n",
    "                # for tag, value in model.named_parameters():\n",
    "                #     tag = tag.replace('.', '/')\n",
    "                #     writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                #     writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "        val_score = eval_model(model, val_loader, device)\n",
    "        scheduler.step()\n",
    "        print('Validation loss: {}'.format(val_score))\n",
    "\n",
    "        #tensorboard\n",
    "        writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "        writer.add_scalar('Validation Loss', val_score, global_step)\n",
    "        writer.add_images('images', imgs, global_step)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes['train']\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes['train']\n",
    "        print(' epoch {}, Loss: {:.4f} Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
    "        writer.add_scalar(\"epoch loss\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"epoch acc\", epoch_acc, epoch)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                print('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(model.state_dict(),\n",
    "                       dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n",
    "            print(f'Checkpoint {epoch + 1} saved !')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 0/4\n",
      "Epoch 0 , Train Step 0, Training loss: 1.7884879112243652\n",
      "Epoch 0 , Train Step 10, Training loss: 1.7266604900360107\n",
      "Epoch 0 , Train Step 20, Training loss: 1.8178170919418335\n",
      "Epoch 0 , Train Step 30, Training loss: 1.704101800918579\n",
      "Epoch 0 , Train Step 40, Training loss: 1.644623875617981\n",
      "Epoch 0 , Train Step 50, Training loss: 1.5260205268859863\n",
      "Epoch 0 , Train Step 60, Training loss: 1.713887333869934\n",
      "Epoch 0 , Train Step 70, Training loss: 1.8749359846115112\n",
      "Epoch 0 , Train Step 80, Training loss: 1.4829652309417725\n",
      "Epoch 0 , Train Step 90, Training loss: 1.7189066410064697\n",
      "Epoch 0 , Train Step 100, Training loss: 1.5081465244293213\n",
      "Epoch 0 , Train Step 110, Training loss: 1.6298831701278687\n",
      "Epoch 0 , Train Step 120, Training loss: 1.7084238529205322\n",
      "Epoch 0 , Train Step 130, Training loss: 1.798529028892517\n",
      "Epoch 0 , Train Step 140, Training loss: 1.6502529382705688\n",
      "Epoch 0 , Train Step 150, Training loss: 1.5643610954284668\n",
      "Epoch 0 , Train Step 160, Training loss: 1.8554314374923706\n",
      "Epoch 0 , Train Step 170, Training loss: 1.7033157348632812\n",
      "Epoch 0 , Train Step 180, Training loss: 1.625972867012024\n",
      "Epoch 0 , Train Step 190, Training loss: 1.7030479907989502\n",
      "Epoch 0 , Train Step 200, Training loss: 1.4902632236480713\n",
      "Epoch 0 , Train Step 210, Training loss: 1.6421840190887451\n",
      "Epoch 0 , Train Step 220, Training loss: 1.6919975280761719\n",
      "Epoch 0 , Train Step 230, Training loss: 1.6844574213027954\n",
      "Epoch 0 , Train Step 240, Training loss: 1.6358098983764648\n",
      "Validation loss: 1.64481552250414\n",
      " epoch 0, Loss: 1.6894 Acc: 0.2830\n",
      "Checkpoint 1 saved !\n",
      "Epoch 1/4\n",
      "Epoch 1 , Train Step 0, Training loss: 1.5772278308868408\n",
      "Epoch 1 , Train Step 10, Training loss: 1.579710602760315\n",
      "Epoch 1 , Train Step 20, Training loss: 1.5238211154937744\n",
      "Epoch 1 , Train Step 30, Training loss: 1.4634891748428345\n",
      "Epoch 1 , Train Step 40, Training loss: 1.6027642488479614\n",
      "Epoch 1 , Train Step 50, Training loss: 1.4857425689697266\n",
      "Epoch 1 , Train Step 60, Training loss: 1.4885616302490234\n",
      "Epoch 1 , Train Step 70, Training loss: 1.584431767463684\n",
      "Epoch 1 , Train Step 80, Training loss: 1.7717232704162598\n",
      "Epoch 1 , Train Step 90, Training loss: 1.6425342559814453\n",
      "Epoch 1 , Train Step 100, Training loss: 1.5260463953018188\n",
      "Epoch 1 , Train Step 110, Training loss: 1.5329333543777466\n",
      "Epoch 1 , Train Step 120, Training loss: 1.737245798110962\n",
      "Epoch 1 , Train Step 130, Training loss: 1.6021162271499634\n",
      "Epoch 1 , Train Step 140, Training loss: 1.7471461296081543\n",
      "Epoch 1 , Train Step 150, Training loss: 1.5164670944213867\n",
      "Epoch 1 , Train Step 160, Training loss: 1.6744545698165894\n",
      "Epoch 1 , Train Step 170, Training loss: 1.6165392398834229\n",
      "Epoch 1 , Train Step 180, Training loss: 1.6108384132385254\n",
      "Epoch 1 , Train Step 190, Training loss: 1.5969507694244385\n",
      "Epoch 1 , Train Step 200, Training loss: 1.7069392204284668\n",
      "Epoch 1 , Train Step 210, Training loss: 1.5994902849197388\n",
      "Epoch 1 , Train Step 220, Training loss: 1.562656044960022\n",
      "Epoch 1 , Train Step 230, Training loss: 1.40487802028656\n",
      "Epoch 1 , Train Step 240, Training loss: 1.6416114568710327\n",
      "Validation loss: 1.639163624809449\n",
      " epoch 1, Loss: 1.6145 Acc: 0.3241\n",
      "Checkpoint 2 saved !\n",
      "Epoch 2/4\n",
      "Epoch 2 , Train Step 0, Training loss: 1.829025387763977\n",
      "Epoch 2 , Train Step 10, Training loss: 1.5421137809753418\n",
      "Epoch 2 , Train Step 20, Training loss: 1.7784249782562256\n",
      "Epoch 2 , Train Step 30, Training loss: 1.369957447052002\n",
      "Epoch 2 , Train Step 40, Training loss: 1.5206451416015625\n",
      "Epoch 2 , Train Step 50, Training loss: 1.4489500522613525\n",
      "Epoch 2 , Train Step 60, Training loss: 1.5126566886901855\n",
      "Epoch 2 , Train Step 70, Training loss: 1.687662959098816\n",
      "Epoch 2 , Train Step 80, Training loss: 1.6554516553878784\n",
      "Epoch 2 , Train Step 90, Training loss: 1.380136489868164\n",
      "Epoch 2 , Train Step 100, Training loss: 1.5622648000717163\n",
      "Epoch 2 , Train Step 110, Training loss: 1.6607125997543335\n",
      "Epoch 2 , Train Step 120, Training loss: 1.2634272575378418\n",
      "Epoch 2 , Train Step 130, Training loss: 1.4816523790359497\n",
      "Epoch 2 , Train Step 140, Training loss: 1.4115146398544312\n",
      "Epoch 2 , Train Step 150, Training loss: 1.783605694770813\n",
      "Epoch 2 , Train Step 160, Training loss: 1.4917153120040894\n",
      "Epoch 2 , Train Step 170, Training loss: 1.2557306289672852\n",
      "Epoch 2 , Train Step 180, Training loss: 1.4836173057556152\n",
      "Epoch 2 , Train Step 190, Training loss: 1.1635534763336182\n",
      "Epoch 2 , Train Step 200, Training loss: 1.7657495737075806\n",
      "Epoch 2 , Train Step 210, Training loss: 1.4620118141174316\n",
      "Epoch 2 , Train Step 220, Training loss: 1.5510163307189941\n",
      "Epoch 2 , Train Step 230, Training loss: 1.463106632232666\n",
      "Epoch 2 , Train Step 240, Training loss: 1.6813771724700928\n",
      "Validation loss: 1.6375055787074997\n",
      " epoch 2, Loss: 1.5455 Acc: 0.3687\n",
      "Checkpoint 3 saved !\n",
      "Epoch 3/4\n",
      "Epoch 3 , Train Step 0, Training loss: 1.2757363319396973\n",
      "Epoch 3 , Train Step 10, Training loss: 1.4295684099197388\n",
      "Epoch 3 , Train Step 20, Training loss: 1.3700525760650635\n",
      "Epoch 3 , Train Step 30, Training loss: 1.4975368976593018\n",
      "Epoch 3 , Train Step 40, Training loss: 1.243109107017517\n",
      "Epoch 3 , Train Step 50, Training loss: 1.6093790531158447\n",
      "Epoch 3 , Train Step 60, Training loss: 1.7139357328414917\n",
      "Epoch 3 , Train Step 70, Training loss: 1.6035195589065552\n",
      "Epoch 3 , Train Step 80, Training loss: 1.6564974784851074\n",
      "Epoch 3 , Train Step 90, Training loss: 1.5395936965942383\n",
      "Epoch 3 , Train Step 100, Training loss: 1.553144097328186\n",
      "Epoch 3 , Train Step 110, Training loss: 1.2421457767486572\n",
      "Epoch 3 , Train Step 120, Training loss: 1.7728798389434814\n",
      "Epoch 3 , Train Step 130, Training loss: 1.4355413913726807\n",
      "Epoch 3 , Train Step 140, Training loss: 1.5162920951843262\n",
      "Epoch 3 , Train Step 150, Training loss: 1.612446904182434\n",
      "Epoch 3 , Train Step 160, Training loss: 1.5432881116867065\n",
      "Epoch 3 , Train Step 170, Training loss: 1.5178074836730957\n",
      "Epoch 3 , Train Step 180, Training loss: 1.5309739112854004\n",
      "Epoch 3 , Train Step 190, Training loss: 1.5119192600250244\n",
      "Epoch 3 , Train Step 200, Training loss: 1.4798952341079712\n",
      "Epoch 3 , Train Step 210, Training loss: 1.6066558361053467\n",
      "Epoch 3 , Train Step 220, Training loss: 1.4044559001922607\n",
      "Epoch 3 , Train Step 230, Training loss: 1.400582194328308\n",
      "Epoch 3 , Train Step 240, Training loss: 1.30838143825531\n",
      "Validation loss: 1.6122576314282704\n",
      " epoch 3, Loss: 1.4868 Acc: 0.4069\n",
      "Checkpoint 4 saved !\n",
      "Epoch 4/4\n",
      "Epoch 4 , Train Step 0, Training loss: 1.484621286392212\n",
      "Epoch 4 , Train Step 10, Training loss: 1.3333775997161865\n",
      "Epoch 4 , Train Step 20, Training loss: 1.4210847616195679\n",
      "Epoch 4 , Train Step 30, Training loss: 1.4512879848480225\n",
      "Epoch 4 , Train Step 40, Training loss: 1.3119707107543945\n",
      "Epoch 4 , Train Step 50, Training loss: 1.3794697523117065\n",
      "Epoch 4 , Train Step 60, Training loss: 1.1634113788604736\n",
      "Epoch 4 , Train Step 70, Training loss: 1.471287727355957\n",
      "Epoch 4 , Train Step 80, Training loss: 1.469111680984497\n",
      "Epoch 4 , Train Step 90, Training loss: 1.2298380136489868\n",
      "Epoch 4 , Train Step 100, Training loss: 1.3063485622406006\n",
      "Epoch 4 , Train Step 110, Training loss: 1.4193837642669678\n",
      "Epoch 4 , Train Step 120, Training loss: 1.5151814222335815\n",
      "Epoch 4 , Train Step 130, Training loss: 1.534081220626831\n",
      "Epoch 4 , Train Step 140, Training loss: 1.3310377597808838\n",
      "Epoch 4 , Train Step 150, Training loss: 1.1480309963226318\n",
      "Epoch 4 , Train Step 160, Training loss: 1.2660298347473145\n",
      "Epoch 4 , Train Step 170, Training loss: 1.5072011947631836\n",
      "Epoch 4 , Train Step 180, Training loss: 1.3067007064819336\n",
      "Epoch 4 , Train Step 190, Training loss: 1.2619965076446533\n",
      "Epoch 4 , Train Step 200, Training loss: 1.3159435987472534\n",
      "Epoch 4 , Train Step 210, Training loss: 1.4562140703201294\n",
      "Epoch 4 , Train Step 220, Training loss: 1.368301510810852\n",
      "Epoch 4 , Train Step 230, Training loss: 1.8734911680221558\n",
      "Epoch 4 , Train Step 240, Training loss: 1.3745472431182861\n",
      "Validation loss: 1.625771176384156\n",
      " epoch 4, Loss: 1.4219 Acc: 0.4437\n",
      "Checkpoint 5 saved !\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device='cpu'\n",
    "num_class = 6\n",
    "\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "# Comment out to finetune further\n",
    "for l in model.base_layers:\n",
    "    for param in l.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)        \n",
    "        \n",
    "# model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=1)\n",
    "\n",
    "model = new_train_model(model, device, epochs=5, batch_size=32, lr=0.001,val_percent=0.1, save_cp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_loader) :\n",
    "    print(batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Resize, ToPILImage\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import os \n",
    "import skimage \n",
    "from skimage import io \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, csv_file, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"        \n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "\n",
    "        ## Local images only  \n",
    "        \n",
    "        # local_files_df = DataFrame([os.path.splitext(filename)[0] for filename in os.listdir('/Users/abharani/Documents/myworkspace/cs231n_project/data/train_images/') if filename.endswith(\".tiff\")], columns=[\"image_id\"])\n",
    "        # self.data_frame = local_files_df.join(data_frame.set_index('image_id'), on='image_id')\n",
    "        # ##\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        logging.info('Creating dataset with {} examples'.format(len(self.data_frame)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, image):\n",
    "        WINDOW_SIZE = 128\n",
    "        STRIDE = 64\n",
    "        K = 16\n",
    "        \n",
    "        image, best_coordinates, best_regions = generate_patches(image, window_size=WINDOW_SIZE, stride=STRIDE, k=K)\n",
    "        glued_image = glue_to_one_picture(best_regions, window_size=WINDOW_SIZE, k=K)\n",
    "        \n",
    "        return glued_image\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_id = self.data_frame.iloc[idx]['image_id']\n",
    "        image_file_path = os.path.join(os.path.join(self.data_dir,'train_images'),image_id + \".tiff\")\n",
    "        image = skimage.io.MultiImage(image_file_path)[-1]\n",
    "        image = np.array(image)\n",
    "\n",
    "        isup_grade = self.data_frame.iloc[idx]['isup_grade']\n",
    "\n",
    "        image = self.preprocess(image)\n",
    "        PIL_image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "#         print(\"Image type {}\".format(image.type)) #numpy.ndarray\n",
    "#         print(PIL_image.type)                     # Image object      \n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            \n",
    "            image = self.transform(PIL_image)\n",
    "        \n",
    "        return {'image': image, 'isup_grade' : isup_grade}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transform=Compose([Resize((224,224)),ToTensor(), Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]) \n",
    "                                                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10616"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = BasicDataset(csv_file='/project/data/train.csv', data_dir='/project/data',transform=my_transform)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([3, 224, 224]) 0\n",
      "1 torch.Size([3, 224, 224]) 0\n",
      "2 torch.Size([3, 224, 224]) 4\n",
      "3 torch.Size([3, 224, 224]) 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(datasets['train'])):\n",
    "    sample = dataset[i]\n",
    "\n",
    "    print(i, sample['image'].shape, sample['isup_grade'])\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10616\n",
      "Generating Small Train/Valid Dataset\n",
      "200\n",
      "50\n",
      "<torch.utils.data.dataset.Subset object at 0x7f69d44d06d0>\n"
     ]
    }
   ],
   "source": [
    "def train_val_dataset(dataset, val_split=0.25, generate_small=False):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    if generate_small:\n",
    "        print(\"Generating Small Train/Valid Dataset\")\n",
    "        datasets['train'] = Subset(dataset, train_idx[0:200])\n",
    "        datasets['val'] = Subset(dataset, val_idx[0:50])\n",
    "    else:\n",
    "        datasets['train'] = Subset(dataset, train_idx)\n",
    "        datasets['val'] = Subset(dataset, val_idx)        \n",
    "    return datasets\n",
    "\n",
    "# dataset = ImageFolder(os.path.join(data_dir,'output'), transform=Compose([Resize((224,224)),ToTensor(), \n",
    "#                                                                       Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                                                                std=[0.229, 0.224, 0.225]) \n",
    "#                                                                       ]))\n",
    "print(len(dataset))\n",
    "datasets = train_val_dataset(dataset,generate_small=True)\n",
    "print(len(datasets['train']))\n",
    "print(len(datasets['val']))\n",
    "# The original dataset is available in the Subset class\n",
    "print(datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(image):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image                  numpy.array   multi-dimensional array of the form WxHxC\n",
    "    \n",
    "    Returns:\n",
    "        ratio_white_pixels     float         ratio of white pixels over total pixels in the image \n",
    "    \"\"\"\n",
    "    width, height = image.shape[0], image.shape[1]\n",
    "    num_pixels = width * height\n",
    "    \n",
    "    num_white_pixels = 0\n",
    "    \n",
    "    summed_matrix = np.sum(image, axis=-1)\n",
    "    # Note: A 3-channel white pixel has RGB (255, 255, 255)\n",
    "    num_white_pixels = np.count_nonzero(summed_matrix > 620)\n",
    "    ratio_white_pixels = num_white_pixels / num_pixels\n",
    "    \n",
    "    green_concentration = np.mean(image[1])\n",
    "    blue_concentration = np.mean(image[2])\n",
    "    \n",
    "    return ratio_white_pixels, green_concentration, blue_concentration\n",
    "\n",
    "def select_k_best_regions(regions, k=20):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        regions               list           list of 2-component tuples first component the region, \n",
    "                                             second component the ratio of white pixels\n",
    "                                             \n",
    "        k                     int            number of regions to select\n",
    "    \"\"\"\n",
    "    regions = [x for x in regions if x[3] > 180 and x[4] > 180]\n",
    "    k_best_regions = sorted(regions, key=lambda tup: tup[2])[:k]\n",
    "    return k_best_regions\n",
    "\n",
    "def generate_patches(image, window_size=200, stride=128, k=20):\n",
    "    \n",
    "#     image = skimage.io.MultiImage(slide_path)[-2]\n",
    "#     image = np.array(image)\n",
    "    \n",
    "    max_width, max_height = image.shape[0], image.shape[1]\n",
    "    regions_container = []\n",
    "    i = 0\n",
    "    \n",
    "    while window_size + stride*i <= max_height:\n",
    "        j = 0\n",
    "        \n",
    "        while window_size + stride*j <= max_width:            \n",
    "            x_top_left_pixel = j * stride\n",
    "            y_top_left_pixel = i * stride\n",
    "            \n",
    "            patch = image[\n",
    "                x_top_left_pixel : x_top_left_pixel + window_size,\n",
    "                y_top_left_pixel : y_top_left_pixel + window_size,\n",
    "                :\n",
    "            ]\n",
    "            \n",
    "            ratio_white_pixels, green_concentration, blue_concentration = compute_statistics(patch)\n",
    "            \n",
    "            region_tuple = (x_top_left_pixel, y_top_left_pixel, ratio_white_pixels, green_concentration, blue_concentration)\n",
    "            regions_container.append(region_tuple)\n",
    "            \n",
    "            j += 1\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    k_best_region_coordinates = select_k_best_regions(regions_container, k=k)\n",
    "    k_best_regions = get_k_best_regions(k_best_region_coordinates, image, window_size)\n",
    "    \n",
    "    return image, k_best_region_coordinates, k_best_regions\n",
    "\n",
    "def display_images(regions, title):\n",
    "    fig, ax = plt.subplots(5, 4, figsize=(15, 15))\n",
    "    \n",
    "    for i, region in regions.items():\n",
    "        ax[i//4, i%4].imshow(region)\n",
    "    \n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    \n",
    "def get_k_best_regions(coordinates, image, window_size=512):\n",
    "    regions = {}\n",
    "    for i, tup in enumerate(coordinates):\n",
    "        x, y = tup[0], tup[1]\n",
    "        regions[i] = image[x : x+window_size, y : y+window_size, :]\n",
    "    \n",
    "    return regions\n",
    "\n",
    "\n",
    "def glue_to_one_picture(image_patches, window_size=200, k=16):\n",
    "    side = int(np.sqrt(k))\n",
    "    image = np.zeros((side*window_size, side*window_size, 3), dtype=np.int16)\n",
    "        \n",
    "    for i, patch in image_patches.items():\n",
    "        x = i // side\n",
    "        y = i % side\n",
    "        image[\n",
    "            x * window_size : (x+1) * window_size,\n",
    "            y * window_size : (y+1) * window_size,\n",
    "            :\n",
    "        ] = patch\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bears = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock), \n",
    "    get_items=get_image_files, \n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    get_y=parent_label,\n",
    "    item_tfms=Resize(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.widgets import *\n",
    "from fastai2.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = bears.dataloaders(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.show_batch(max_n=1,nrows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3),batch_tfms=aug_transforms(mult=2))\n",
    "dls = bears.dataloaders(path)\n",
    "dls.valid.show_batch(max_n=4, nrows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bears = bears.new(\n",
    "    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n",
    "    batch_tfms=aug_transforms())\n",
    "dls = bears.dataloaders(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, model, metrics=error_rate)\n",
    "learn.fine_tune(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs231n]",
   "language": "python",
   "name": "conda-env-cs231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
