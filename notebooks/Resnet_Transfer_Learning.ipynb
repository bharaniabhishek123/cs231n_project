{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import slidingwindow as sw \n",
    "import skimage \n",
    "from skimage import io \n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import re #2 \n",
    "import random #3\n",
    "from time import time #4\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import torch # 7\n",
    "from torch.utils.data  import Dataset # 8\n",
    "\n",
    "from fastai2.data.core import DataLoaders # 5\n",
    "from fastai2.vision.all import * #6\n",
    "from fastai2.vision.widgets import *\n",
    "from fastai2.data.external import untar_data,URLs\n",
    "from fastai2.data.transforms import get_image_files # 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = \"/project/data/\"   # if on gcloud \n",
    "\n",
    "train = pd.read_csv(os.path.join(BASE_FOLDER, 'train.csv'))\n",
    "\n",
    "image_dir = '/home/abharani/data/train_images/'\n",
    "path = Path(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Wrapping all pre-processing (resize, conversion to tensor, dividing by 255 and reordering of the channels) on image into one step using a helper func.\n",
    "2. See label for image inside the file name (or generate via image_id from fname) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_image(fname):\n",
    "    WINDOW_SIZE = 256\n",
    "    STRIDE = 64\n",
    "    K = 16\n",
    "    image = skimage.io.MultiImage(str(fname))[-1]\n",
    "    midres_image, best_coordinates, best_regions_image = generate_patches(image, window_size=WINDOW_SIZE, stride=STRIDE, k=K)\n",
    "    glued_image = glue_to_one_picture(best_regions_image, window_size=WINDOW_SIZE, k=K)\n",
    "    t = torch.Tensor(np.array(glued_image))\n",
    "    return t.permute(2,0,1).float()/255.0\n",
    "\n",
    "\n",
    "def label_func1(fname):\n",
    "    return re.match(r'^(.*)_\\d+.jpg$', fname.name).groups()[0]\n",
    "\n",
    "def label_func2(filepath):\n",
    "    \"\"\" input : FilePath\n",
    "    \"\"\"\n",
    "    print(filepath)\n",
    "    file_path = os.path.splitext(filepath)[0]\n",
    "    image_id = file_path.split(\"/\")[-1]    \n",
    "    \n",
    "    return train.loc[train['image_id']==image_id]['isup_grade'].values[0]\n",
    "\n",
    "\n",
    "\n",
    "def label_func3(file_path):\n",
    "    \"\"\" input : file_path (path:object)\n",
    "                ( e.g. /home/abharani/data/train_images/6fc63d2394ebade5d7e09856eab1f726_0.jpg)\n",
    "        returns : 1 , 2, 3 (int)\n",
    "    \"\"\" \n",
    "    image_name = str(file_path).split(\"/\")[-1]\n",
    "\n",
    "    return int(image_name.replace(\".jpg\", \"\").split(\"_\")[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate list of files at image dir, pick random indexes and perform split for train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_image_files(path)[0:200]\n",
    "idxs = np.random.permutation(range(len(files)))\n",
    "cut = int(0.8 * len(files))\n",
    "train_files = files[idxs[:cut]]\n",
    "valid_files = files[idxs[cut:]]\n",
    "print(\"Training set images {}, Validation set images {}\".format(len(train_files),len(valid_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check unique lables in dataset and distribution of each label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(files.map(label_func3)))\n",
    "print(\"distinct labels {}\".format(len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach I  - Purely Pytorch \n",
    "Following from https://dev.fast.ai/tutorial.siamese\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We can use above files to create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiopsyDataset(Dataset):\n",
    "    def __init__(self, files, is_valid=False):\n",
    "        self.files = files\n",
    "        self.is_valid =is_valid\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        file_path = self.files[i]\n",
    "        tic = time.time()\n",
    "        processed_image = pre_process_image(file_path) \n",
    "        toc = time.time()\n",
    "        print(\"Time took to pre-process {} secs\".format(toc-tic))\n",
    "        cls = label_func3(file_path)\n",
    "        y_tensor = torch.tensor(cls, dtype=torch.long)\n",
    "        return (processed_image, y_tensor)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.files)\n",
    "    \n",
    "    \n",
    "train_ds :Dataset = BiopsyDataset(train_files)\n",
    "valid_ds :Dataset = BiopsyDataset(valid_files, is_valid=True)\n",
    "\n",
    "# Validate dataset\n",
    "\n",
    "for i in range(len(train_ds)):\n",
    "    sample = train_ds[i]\n",
    "\n",
    "    print(i, sample[0].shape, sample[1])\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataLoaders with the following factory method DataLoaders\n",
    "\n",
    "We can change batch-size depending upon gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dsets(train_ds, valid_ds,bs=5,num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to use the GPU and inspect one batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dls.cuda()\n",
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create cnn_learner using pre-trained resnet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet50, metrics=[accuracy],n_out=6,loss_func=F.cross_entropy)\n",
    "learn.fine_tune(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Approach I\n",
    "what is a bit annoying is that we have to rewrite everything that is already in fastai if we want to normalize our images, or apply data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach II - Fastai\n",
    "Following from https://dev.fast.ai/tutorial.siamese\n",
    "\n",
    "A dataset like before, you can easily convert it into a fastai Transform by just changing the __getitem__ function to encodes. \n",
    "\n",
    "So three things changed:\n",
    "\n",
    "1. the __len__ disappeared, we won't need it\n",
    "2. __getitem___ became encodes\n",
    "3. we return TensorImage for our images\n",
    "\n",
    "still wrapping all pre-processing (resize, conversion to tensor, dividing by 255 and reordering of the channels) on image into one step using a helper func. \n",
    "and generating label for image inside the file name (or generate via image_id from fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiopsyTransform(Transform):\n",
    "    def __init__(self, files, is_valid=False):\n",
    "        self.files = files\n",
    "        self.is_valid = is_valid\n",
    "        \n",
    "    def encodes(self, i):\n",
    "        file_path = self.files[i]\n",
    "#         tic = time.time()\n",
    "        processed_image = pre_process_image(file_path) \n",
    "#         toc = time.time()\n",
    "#         print(\"Time took to pre-process {} secs\".format(toc-tic)) \n",
    "        cls = label_func3(file_path)\n",
    "        y_tensor = torch.tensor(cls, dtype=torch.long)\n",
    "        return (TensorImage(processed_image), y_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do we build a dataset with this? We will use TfmdLists. It's just an object that lazily applies a collection of Transforms on a list. Here since our transform takes integers, we will pass simple ranges for this list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tl= TfmdLists(range(len(train_files)), BiopsyTransform(train_files))\n",
    "valid_tl= TfmdLists(range(len(valid_files)), BiopsyTransform(valid_files, is_valid=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then, when we create a DataLoader, we can add any transform we like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dsets(train_tl, valid_tl, bs=5,num_workers=4,after_item=[Resize(224), ToTensor],\n",
    "                             after_batch=[Resize(224),Normalize.from_stats(*imagenet_stats), *aug_transforms()])\n",
    "dls = dls.cuda()\n",
    "b = dls.one_batch()\n",
    "print(b[0].shape,b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, sample in enumerate(dls):\n",
    "#     print(sample)\n",
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create cnn_learner using pre-trained resnet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet50, metrics=[accuracy],n_out=6,loss_func=F.cross_entropy)\n",
    "learn.fine_tune(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()\n",
    "path.ls(file_exts='.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_inf = load_learner(path/'export_resnet50.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patches(image, window_size=200, stride=128, k=20):\n",
    "    \n",
    "#     image = skimage.io.MultiImage(slide_path)[-2]\n",
    "#     image = np.array(image)\n",
    "    \n",
    "    max_width, max_height = image.shape[0], image.shape[1]\n",
    "    regions_container = []\n",
    "    i = 0\n",
    "    \n",
    "    while window_size + stride*i <= max_height:\n",
    "        j = 0\n",
    "        \n",
    "        while window_size + stride*j <= max_width:            \n",
    "            x_top_left_pixel = j * stride\n",
    "            y_top_left_pixel = i * stride\n",
    "            \n",
    "            patch = image[\n",
    "                x_top_left_pixel : x_top_left_pixel + window_size,\n",
    "                y_top_left_pixel : y_top_left_pixel + window_size,\n",
    "                :\n",
    "            ]\n",
    "            \n",
    "            ratio_white_pixels, green_concentration, blue_concentration = compute_statistics(patch)\n",
    "            \n",
    "            region_tuple = (x_top_left_pixel, y_top_left_pixel, ratio_white_pixels, green_concentration, blue_concentration)\n",
    "            regions_container.append(region_tuple)\n",
    "            \n",
    "            j += 1\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    k_best_region_coordinates = select_k_best_regions(regions_container, k=k)\n",
    "    k_best_regions = get_k_best_regions(k_best_region_coordinates, image, window_size)\n",
    "    \n",
    "    return image, k_best_region_coordinates, k_best_regions\n",
    "\n",
    "\n",
    "def compute_statistics(image):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image                  numpy.array   multi-dimensional array of the form WxHxC\n",
    "    \n",
    "    Returns:\n",
    "        ratio_white_pixels     float         ratio of white pixels over total pixels in the image \n",
    "    \"\"\"\n",
    "    width, height = image.shape[0], image.shape[1]\n",
    "    num_pixels = width * height\n",
    "    \n",
    "    num_white_pixels = 0\n",
    "    \n",
    "    summed_matrix = np.sum(image, axis=-1)\n",
    "    # Note: A 3-channel white pixel has RGB (255, 255, 255)\n",
    "    num_white_pixels = np.count_nonzero(summed_matrix > 620)\n",
    "    ratio_white_pixels = num_white_pixels / num_pixels\n",
    "    \n",
    "    green_concentration = np.mean(image[1])\n",
    "    blue_concentration = np.mean(image[2])\n",
    "    \n",
    "    return ratio_white_pixels, green_concentration, blue_concentration\n",
    "\n",
    "def select_k_best_regions(regions, k=20):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        regions               list           list of 2-component tuples first component the region, \n",
    "                                             second component the ratio of white pixels\n",
    "                                             \n",
    "        k                     int            number of regions to select\n",
    "    \"\"\"\n",
    "    regions = [x for x in regions if x[3] > 180 and x[4] > 180]\n",
    "    k_best_regions = sorted(regions, key=lambda tup: tup[2])[:k]\n",
    "    return k_best_regions\n",
    "\n",
    "\n",
    "def display_images(regions, title):\n",
    "    fig, ax = plt.subplots(5, 4, figsize=(15, 15))\n",
    "    \n",
    "    for i, region in regions.items():\n",
    "        ax[i//4, i%4].imshow(region)\n",
    "    \n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    \n",
    "def get_k_best_regions(coordinates, image, window_size=512):\n",
    "    regions = {}\n",
    "    for i, tup in enumerate(coordinates):\n",
    "        x, y = tup[0], tup[1]\n",
    "        regions[i] = image[x : x+window_size, y : y+window_size, :]\n",
    "    \n",
    "    return regions\n",
    "\n",
    "\n",
    "def glue_to_one_picture(image_patches, window_size=200, k=16):\n",
    "    side = int(np.sqrt(k))\n",
    "    image = np.zeros((side*window_size, side*window_size, 3), dtype=np.int16)\n",
    "        \n",
    "    for i, patch in image_patches.items():\n",
    "        x = i // side\n",
    "        y = i % side\n",
    "        image[\n",
    "            x * window_size : (x+1) * window_size,\n",
    "            y * window_size : (y+1) * window_size,\n",
    "            :\n",
    "        ] = patch\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs231n]",
   "language": "python",
   "name": "conda-env-cs231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
