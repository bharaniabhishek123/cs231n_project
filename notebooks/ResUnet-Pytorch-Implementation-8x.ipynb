{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be using pytorch datasets and data loaders to implement  residual U-net model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset, sampler, Subset, WeightedRandomSampler\n",
    "\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import skimage \n",
    "from skimage import io \n",
    "import logging\n",
    "from PIL import Image\n",
    "\n",
    "import os \n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, ToPILImage\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000  # incase PIL gives error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VM , glued images are stored at path '/home/abharani/cs231n_project/glued_images/data'\n",
    "root_dir = '/home/abharani/cs231n_project'\n",
    "data_dir = '/home/abharani/cs231n_project/data'\n",
    "\n",
    "# Local \n",
    "# root_dir = '/Users/abharani/Documents/myworkspace/cs231n_project'\n",
    "# data_dir = '/Users/abharani/Documents/myworkspace/cs231n_project/data'\n",
    "\n",
    "\n",
    "dir_checkpoint = os.path.join(root_dir, 'checkpoints')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 :  Train , Valid and Test Split \n",
    "\n",
    "Afrer saving the images generated from Dataset class ImageFolderWithPaths. Create dataset using the ImageFolder.\n",
    "Since Data is imbalanced, performing Weighted Random Sampling not Random Sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.25, generate_small=False):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets_created = {}\n",
    "    if generate_small:\n",
    "        print(\"Generating Small Train, Test Dataset\")\n",
    "        datasets_created['train'] = Subset(dataset, train_idx[0:200])\n",
    "        datasets_created['test'] = Subset(dataset, val_idx[0:50])\n",
    "    else:\n",
    "        datasets_created['train'] = Subset(dataset, train_idx)\n",
    "        datasets_created['test'] = Subset(dataset, val_idx)        \n",
    "    return datasets_created\n",
    "\n",
    "dataset = ImageFolder('/home/abharani/cs231n_project/glued_images/data',\n",
    "                      transform=Compose([Resize((224,224)),ToTensor(), \n",
    "                                        Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                   std=[0.229, 0.224, 0.225])]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Train, Valid and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset set size 10615\n",
      "Train set size 7164\n",
      "Validation set size 2389\n",
      "Test set size 1062\n"
     ]
    }
   ],
   "source": [
    "dataset_final = {}\n",
    "datasets_created_trial_1 = train_val_dataset(dataset,val_split=0.10,generate_small=False)\n",
    "datasets_created_trial_2 = train_val_dataset(datasets_created_trial_1['train'],val_split=0.25,generate_small=False)\n",
    "\n",
    "dataset_final['test'] = datasets_created_trial_1['test']\n",
    "dataset_final['train'] = datasets_created_trial_2['train']\n",
    "dataset_final['val'] = datasets_created_trial_2['test']\n",
    "\n",
    "print(\"Dataset set size {}\".format(len(dataset)))\n",
    "print(\"Train set size {}\".format(len(dataset_final['train'])))\n",
    "print(\"Validation set size {}\".format(len(dataset_final['val'])))\n",
    "print(\"Test set size {}\".format(len(dataset_final['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(datasets['train'])):\n",
    "#     sample = dataset[i]\n",
    "\n",
    "#     print(i, sample['image'].shape, sample['isup_grade'])\n",
    "\n",
    "#     if i == 3:\n",
    "#         plt.show()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: WeightedRandomSampling on Imbalance train set\n",
    "\n",
    "Performing WeightedRandomSampling using link : https://towardsdatascience.com/pytorch-basics-sampling-samplers-2a0f29f0bf2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate class weights and target_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes: \n",
      " defaultdict(<class 'int'>, {2: 912, 4: 831, 0: 1956, 1: 1813, 3: 814, 5: 838})\n",
      "Class weights : \n",
      " tensor([0.0011, 0.0012, 0.0005, 0.0006, 0.0012, 0.0012])\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "count_dict = defaultdict(int)\n",
    "target_list = []\n",
    "\n",
    "#Generate target_list of all labels and count dict of all classes\n",
    "\n",
    "for i, (image, label) in enumerate(dataset_final['train']):    \n",
    "    count_dict[label] += 1\n",
    "    target_list.append(label)\n",
    "    \n",
    "#     print(i, image.shape,sample)    \n",
    "#     if i== 3:\n",
    "#         break\n",
    "\n",
    "\n",
    "count_dict\n",
    "print(\"Distribution of classes: \\n\", count_dict)\n",
    "\n",
    "class_count = [i for i in count_dict.values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "print(\"Class weights : \\n\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = torch.tensor(target_list)\n",
    "target_list = target_list[torch.randperm(len(target_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0011, 0.0012, 0.0006,  ..., 0.0012, 0.0005, 0.0006])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights_all = class_weights[target_list]\n",
    "class_weights_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use gpu and float as dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device set to cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(\"device set to {}\".format(device))\n",
    "    \n",
    "\n",
    "base_model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# list(base_model.children())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "        self.base_layers = list(base_model.children())                \n",
    "        \n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)  \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        # MY experiment\n",
    "        self.fc = nn.Linear(301056, 6)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(input)            \n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "\n",
    "        # MY experiment\n",
    "        x = out.view(out.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # x = F.adaptive_avg_pool2d(out, output_size=1)\n",
    "        # print(x.size(),x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use new_train_enhanced_images below for training on images glued ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with 8.x image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.6697 Acc: 0.2994\n",
      "val Loss: 1.6505 Acc: 0.2842\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.5154 Acc: 0.3836\n",
      "val Loss: 1.6298 Acc: 0.3361\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3108 Acc: 0.5021\n",
      "val Loss: 1.7994 Acc: 0.3010\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.9477 Acc: 0.6830\n",
      "val Loss: 2.1027 Acc: 0.2901\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.6088 Acc: 0.8131\n",
      "val Loss: 2.5375 Acc: 0.2813\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.3845 Acc: 0.8911\n",
      "val Loss: 2.9201 Acc: 0.2784\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.2223 Acc: 0.9443\n",
      "val Loss: 3.3108 Acc: 0.2825\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-1c2664f5b9f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-1a5d4f162548>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_class = 6\n",
    "\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "# Comment out to finetune further\n",
    "# for l in model.base_layers:\n",
    "#     for param in l.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)        \n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(dataset_final['train'],batch_size=32, sampler = weighted_sampler, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(dataset_final['val'],batch_size=32, num_workers=4, pin_memory=True)\n",
    "\n",
    "dataloaders  = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "dataset_sizes = {x: len(dataset_final[x]) for x in dataset_final.keys()}\n",
    "\n",
    "\n",
    "\n",
    "model = train_model(model, criterion, optimizer_ft, scheduler, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs231n]",
   "language": "python",
   "name": "conda-env-cs231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
