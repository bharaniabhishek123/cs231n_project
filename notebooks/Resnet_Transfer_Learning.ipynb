{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import slidingwindow as sw \n",
    "import skimage \n",
    "from skimage import io \n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import re #2 \n",
    "import random #3\n",
    "from time import time #4\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import torch # 7\n",
    "from torch.utils.data  import Dataset # 8\n",
    "\n",
    "from fastai2.data.core import DataLoaders # 5\n",
    "from fastai2.vision.all import * #6\n",
    "from fastai2.vision.widgets import *\n",
    "from fastai2.data.external import untar_data,URLs\n",
    "from fastai2.data.transforms import get_image_files # 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = \"/project/data/\"   # if on gcloud \n",
    "\n",
    "train = pd.read_csv(os.path.join(BASE_FOLDER, 'train.csv'))\n",
    "\n",
    "image_dir = '/home/abharani/data/train_images/'\n",
    "path = Path(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Wrapping all pre-processing (resize, conversion to tensor, dividing by 255 and reordering of the channels) on image into one step using a helper func.\n",
    "2. See label for image inside the file name (or generate via image_id from fname) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate list of files at image dir, pick random indexes and perform split for train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_image_files(path)[0:200]\n",
    "idxs = np.random.permutation(range(len(files)))\n",
    "cut = int(0.8 * len(files))\n",
    "train_files = files[idxs[:cut]]\n",
    "valid_files = files[idxs[cut:]]\n",
    "print(\"Training set images {}, Validation set images {}\".format(len(train_files),len(valid_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check unique lables in dataset and distribution of each label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(files.map(label_func3)))\n",
    "print(\"distinct labels {}\".format(len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach I  - Purely Pytorch \n",
    "Following from https://dev.fast.ai/tutorial.siamese\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We can use above files to create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiopsyDataset(Dataset):\n",
    "    def __init__(self, files, is_valid=False):\n",
    "        self.files = files\n",
    "        self.is_valid =is_valid\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        file_path = self.files[i]\n",
    "        tic = time.time()\n",
    "        processed_image = pre_process_image(file_path) \n",
    "        toc = time.time()\n",
    "        print(\"Time took to pre-process {} secs\".format(toc-tic))\n",
    "        cls = label_func3(file_path)\n",
    "        y_tensor = torch.tensor(cls, dtype=torch.long)\n",
    "        return (processed_image, y_tensor)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.files)\n",
    "    \n",
    "    \n",
    "train_ds :Dataset = BiopsyDataset(train_files)\n",
    "valid_ds :Dataset = BiopsyDataset(valid_files, is_valid=True)\n",
    "\n",
    "# Validate dataset\n",
    "\n",
    "for i in range(len(train_ds)):\n",
    "    sample = train_ds[i]\n",
    "\n",
    "    print(i, sample[0].shape, sample[1])\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataLoaders with the following factory method DataLoaders\n",
    "\n",
    "We can change batch-size depending upon gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dsets(train_ds, valid_ds,bs=5,num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to use the GPU and inspect one batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dls.cuda()\n",
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create cnn_learner using pre-trained resnet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet50, metrics=[accuracy],n_out=6,loss_func=F.cross_entropy)\n",
    "learn.fine_tune(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Approach I\n",
    "what is a bit annoying is that we have to rewrite everything that is already in fastai if we want to normalize our images, or apply data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach II - Fastai\n",
    "Following from https://dev.fast.ai/tutorial.siamese\n",
    "\n",
    "A dataset like before, you can easily convert it into a fastai Transform by just changing the __getitem__ function to encodes. \n",
    "\n",
    "So three things changed:\n",
    "\n",
    "1. the __len__ disappeared, we won't need it\n",
    "2. __getitem___ became encodes\n",
    "3. we return TensorImage for our images\n",
    "\n",
    "still wrapping all pre-processing (resize, conversion to tensor, dividing by 255 and reordering of the channels) on image into one step using a helper func. \n",
    "and generating label for image inside the file name (or generate via image_id from fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiopsyTransform(Transform):\n",
    "    def __init__(self, files, is_valid=False):\n",
    "        self.files = files\n",
    "        self.is_valid = is_valid\n",
    "        \n",
    "    def encodes(self, i):\n",
    "        file_path = self.files[i]\n",
    "#         tic = time.time()\n",
    "        processed_image = pre_process_image(file_path) \n",
    "#         toc = time.time()\n",
    "#         print(\"Time took to pre-process {} secs\".format(toc-tic)) \n",
    "        cls = label_func3(file_path)\n",
    "        y_tensor = torch.tensor(cls, dtype=torch.long)\n",
    "        return (TensorImage(processed_image), y_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do we build a dataset with this? We will use TfmdLists. It's just an object that lazily applies a collection of Transforms on a list. Here since our transform takes integers, we will pass simple ranges for this list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tl= TfmdLists(range(len(train_files)), BiopsyTransform(train_files))\n",
    "valid_tl= TfmdLists(range(len(valid_files)), BiopsyTransform(valid_files, is_valid=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then, when we create a DataLoader, we can add any transform we like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dsets(train_tl, valid_tl, bs=5,num_workers=4,after_item=[Resize(224), ToTensor],\n",
    "                             after_batch=[Resize(224),Normalize.from_stats(*imagenet_stats), *aug_transforms()])\n",
    "dls = dls.cuda()\n",
    "b = dls.one_batch()\n",
    "print(b[0].shape,b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, sample in enumerate(dls):\n",
    "#     print(sample)\n",
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create cnn_learner using pre-trained resnet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet50, metrics=[accuracy],n_out=6,loss_func=F.cross_entropy)\n",
    "learn.fine_tune(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()\n",
    "path.ls(file_exts='.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_inf = load_learner(path/'export_resnet50.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach III \n",
    "Using Fast ai on processed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset, sampler,Subset,WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_dataset(dataset, val_split=0.25, generate_small=False):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets_created = {}\n",
    "    if generate_small:\n",
    "        print(\"Generating Small Train, Test Dataset\")\n",
    "        datasets_created['train'] = Subset(dataset, train_idx[0:200])\n",
    "        datasets_created['test'] = Subset(dataset, val_idx[0:50])\n",
    "    else:\n",
    "        datasets_created['train'] = Subset(dataset, train_idx)\n",
    "        datasets_created['test'] = Subset(dataset, val_idx)        \n",
    "    return datasets_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10615\n",
      "Train set size 7164\n",
      "Validation set size 2389\n",
      "Test set size 1062\n"
     ]
    }
   ],
   "source": [
    "dataset = ImageFolder('/home/abharani/cs231n_project/glued_images/data', \n",
    "                      transform=Compose([Resize((224,224)),ToTensor(), \n",
    "                                 Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "dataset_final = {}\n",
    "datasets_created_trial_1 = train_val_dataset(dataset,val_split=0.10,generate_small=False)\n",
    "datasets_created_trial_2 = train_val_dataset(datasets_created_trial_1['train'],val_split=0.25,generate_small=False)\n",
    "\n",
    "dataset_final['test'] = datasets_created_trial_1['test']\n",
    "dataset_final['train'] = datasets_created_trial_2['train']\n",
    "dataset_final['val'] = datasets_created_trial_2['test']\n",
    "\n",
    "\n",
    "print(\"Train set size {}\".format(len(dataset_final['train'])))\n",
    "print(\"Validation set size {}\".format(len(dataset_final['val'])))\n",
    "print(\"Test set size {}\".format(len(dataset_final['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes: \n",
      " defaultdict(<class 'int'>, {1: 1811, 0: 1993, 2: 906, 5: 848, 4: 804, 3: 802})\n"
     ]
    }
   ],
   "source": [
    "count_dict = defaultdict(int)\n",
    "target_list = []\n",
    "\n",
    "#Generate target_list of all labels and count dict of all classes\n",
    "for i, (image, label) in enumerate(dataset_final['train']):\n",
    "    count_dict[label] += 1\n",
    "    target_list.append(label)\n",
    "    \n",
    "#     if i== 3:\n",
    "#         break\n",
    "\n",
    "print(\"Distribution of classes: \\n\", count_dict)\n",
    "\n",
    "class_count = [i for i in count_dict.values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "\n",
    "target_list = torch.tensor(target_list)\n",
    "class_weights_all = class_weights[target_list]\n",
    "\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset_final['train'],batch_size=32, sampler = weighted_sampler, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(dataset_final['val'],batch_size=32, sampler = weighted_sampler, num_workers=4, pin_memory=True)\n",
    "\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'after_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2fd6ac97f6fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/fastcore/utils.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mlog_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34mf'{k} (not in signature)'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxtra_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34mf'{f.__qualname__}.{k}'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlog_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbut\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0minst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_return\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0minit_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'init_args'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0minit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/fastai2/vision/learner.py\u001b[0m in \u001b[0;36mcnn_learner\u001b[0;34m(dls, arch, loss_func, pretrained, cut, splitter, y_range, config, n_out, normalize, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_out\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"`n_out` is not defined, and could not be infered from data, set `dls.c` or pass `n_out`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_add_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_range\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'y_range'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_range'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_cnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cut'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/fastai2/vision/learner.py\u001b[0m in \u001b[0;36m_add_norm\u001b[0;34m(dls, meta, pretrained)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_add_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mafter_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mafter_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stats'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'after_batch'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs231n]",
   "language": "python",
   "name": "conda-env-cs231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
