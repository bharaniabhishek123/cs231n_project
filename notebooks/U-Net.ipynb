{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from statistics import mean\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from PIL import Image\n",
    "import random\n",
    "from skimage import io, transform\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, utils\n",
    "from torch import nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import io, transform\n",
    "import skimage\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageWithMaskDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_filePath, image_dir, mask_dir, transform=None):\n",
    "        train_df = pd.read_csv(csv_file_path)\n",
    "            \n",
    "#         masks = os.listdir(mask_dir)\n",
    "#         masks_df = pd.Series(masks).to_frame()\n",
    "#         masks_df.columns = ['mask_file_name']\n",
    "#         masks_df['image_id'] = masks_df.mask_file_name.apply(lambda x: x.split('_')[0])\n",
    "#         train_df = pd.merge(train_df, masks_df, on='image_id', how='outer')\n",
    "#         del masks_df\n",
    "#         print(f\"There are {len(train_df[train_df.mask_file_name.isna()])} images without a mask.\")\n",
    "\n",
    "#         ## removing items where image mask is null\n",
    "#         train_df = train_df[~train_df.mask_file_name.isna()]        \n",
    "        \n",
    "#         images = os.listdir(image_dir)\n",
    "#         images_df = pd.Series(images).to_frame()\n",
    "#         images_df.columns = ['image_file_name']\n",
    "#         images_df['image_id'] = images_df.image_file_name.apply(lambda x: x.split('.')[0])\n",
    "#         train_df = pd.merge(train_df, images_df, on='image_id', how='outer')\n",
    "#         del images_df \n",
    "#         print(f\"There are {len(train_df[train_df.image_file_name.isna()])} image_ids without a image.\")\n",
    "        \n",
    "#         ## Remove image_id not present in image_dir\n",
    "#         train_df = train_df[~train_df.image_file_name.isna()]                \n",
    "        \n",
    "        self.data_frame = train_df.copy()\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        print('Creating dataset with {} examples'.format(len(self.data_frame)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        image_id = self.data_frame.iloc[idx]['image_id']\n",
    "        image_file_path = os.path.join(self.image_dir, image_id + \".tiff\")\n",
    "        image = skimage.io.MultiImage(image_file_path)[-1]\n",
    "        \n",
    "#         image = skimage.transform.resize(image, (224, 224))                \n",
    "#         image = np.array(image)\n",
    "        \n",
    "        image = transform.resize(image,(256,256))\n",
    "        image = image / 255\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            mask_file_path = os.path.join(self.mask_dir, image_id + \"_mask.tiff\")\n",
    "            mask = skimage.io.MultiImage(mask_file_path)[-1]\n",
    "            \n",
    "            mask = transform.resize(mask,(256,256))\n",
    "            mask = mask / 255\n",
    "    #         print('mask shape', mask.shape)\n",
    "            mask = mask.transpose((2, 0, 1))        \n",
    "            mask = mask[0,:,:]\n",
    "            \n",
    "#         mask = np.expand_dims(mask,axis=-1).transpose((2, 0, 1))\n",
    "        except:\n",
    "            return None\n",
    "                        \n",
    "        isup_grade = self.data_frame.iloc[idx]['isup_grade']\n",
    "\n",
    "        return (torch.from_numpy(image), torch.from_numpy(mask), isup_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 10616 examples\n",
      "0 torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "1 torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "2 torch.Size([3, 256, 256]) torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "image_dir = '/project/data/train_images'\n",
    "mask_dir = '/project/data/train_label_masks'\n",
    "csv_file_path = '/home/abharani/data/train_filtered.csv'\n",
    "\n",
    "# csv_file_path = '/kaggle/input/train-filtered/train_filtered.csv'\n",
    "# # csv_file_path =  '/kaggle/input/prostate-cancer-grade-assessment/train.csv'\n",
    "# image_dir = '/kaggle/input/prostate-cancer-grade-assessment/train_images'\n",
    "# mask_dir = '/kaggle/input/prostate-cancer-grade-assessment/train_label_masks'\n",
    "\n",
    "\n",
    "dataset_with_mask = ImageWithMaskDataset(csv_file_path, image_dir, mask_dir)\n",
    "\n",
    "for i in range(len(dataset_with_mask)):\n",
    "    sample = dataset_with_mask[i]\n",
    "    print(i, sample[0].shape, sample[1].shape)\n",
    "    \n",
    "    if i == 2:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset set size 10616\n",
      "Train set size 7165\n",
      "Validation set size 2389\n",
      "Test set size 1062\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import pandas as pd\n",
    "import os\n",
    "import skimage \n",
    "from skimage import io \n",
    "import skimage.transform\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.25, generate_small=False):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets_created = {}\n",
    "    if generate_small:\n",
    "        print(\"Generating Small Train, Test Dataset\")\n",
    "        datasets_created['train'] = Subset(dataset, train_idx[0:200])\n",
    "        datasets_created['test'] = Subset(dataset, val_idx[0:50])\n",
    "    else:\n",
    "        datasets_created['train'] = Subset(dataset, train_idx)\n",
    "        datasets_created['test'] = Subset(dataset, val_idx)        \n",
    "    return datasets_created\n",
    "\n",
    "\n",
    "\n",
    "dataset_final = {}\n",
    "datasets_created_trial_1 = train_val_dataset(dataset_with_mask,val_split=0.10,generate_small=False)\n",
    "datasets_created_trial_2 = train_val_dataset(datasets_created_trial_1['train'],val_split=0.25,generate_small=False)\n",
    "\n",
    "dataset_final['test'] = datasets_created_trial_1['test']\n",
    "dataset_final['train'] = datasets_created_trial_2['train']\n",
    "dataset_final['val'] = datasets_created_trial_2['test']\n",
    "\n",
    "print(\"Dataset set size {}\".format(len(dataset_with_mask)))\n",
    "print(\"Train set size {}\".format(len(dataset_final['train'])))\n",
    "print(\"Validation set size {}\".format(len(dataset_final['val'])))\n",
    "print(\"Test set size {}\".format(len(dataset_final['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_final['train'],batch_size=32, collate_fn=collate_fn, shuffle = True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(dataset_final['val'],batch_size=32, num_workers=4, collate_fn=collate_fn,pin_memory=True)\n",
    "\n",
    "dataloaders  = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "dataset_sizes = {x: len(dataset_final[x]) for x in dataset_final.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tensor to image\n",
    "def image_convert(image):\n",
    "    image = image.clone().cpu().numpy()\n",
    "    image = image.transpose((1,2,0))\n",
    "    image = (image * 255)\n",
    "    return image\n",
    "\n",
    "def mask_convert(mask):\n",
    "    mask = mask.clone().cpu().detach().numpy()\n",
    "    return np.squeeze(mask)\n",
    "\n",
    "def plot_img(no_):\n",
    "    iter_ = iter(train_loader)\n",
    "    images,masks, _ = next(iter_)\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for idx in range(0,no_):\n",
    "         image = image_convert(images[idx])\n",
    "         plt.subplot(2,no_,idx+1)\n",
    "         plt.imshow(image)\n",
    "    for idx in range(0,no_):\n",
    "         mask = mask_convert(masks[idx])\n",
    "         plt.subplot(2,no_,idx+no_+1)\n",
    "         plt.imshow(mask,cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_img(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=(3,3),padding=1):\n",
    "        super(ConvBlock,self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size,padding=padding,bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels,eps=1e-4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class StackEncoder(nn.Module):\n",
    "    def __init__(self,channel1,channel2,kernel_size=(3,3),padding=1):\n",
    "        super(StackEncoder,self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channel1,channel2,kernel_size,padding),\n",
    "            ConvBlock(channel2,channel2,kernel_size,padding),     \n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        big_out = self.block(x)\n",
    "        poolout = self.maxpool(big_out)\n",
    "        return big_out,poolout\n",
    "     \n",
    "        \n",
    "class StackDecoder(nn.Module):\n",
    "    def __init__(self,big_channel,channel1,channel2,kernel_size=(3,3),padding=1):\n",
    "        super(StackDecoder,self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channel1+big_channel,channel2,kernel_size,padding),\n",
    "            ConvBlock(channel2,channel2,kernel_size,padding),\n",
    "            ConvBlock(channel2,channel2,kernel_size,padding),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x,down_tensor):\n",
    "            _, channels, height, width = down_tensor.size()  \n",
    "            x = F.upsample(x, size=(height, width), mode='bilinear',align_corners=True)\n",
    "            x = torch.cat([x, down_tensor], 1)  #combining channels of  input from encoder and upsampling input\n",
    "            x = self.block(x)\n",
    "            return x\n",
    "        \n",
    "        \n",
    "class Unet256(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(Unet256,self).__init__()\n",
    "        \n",
    "        channel,height,width = input_shape\n",
    "        \n",
    "        self.down1 = StackEncoder(channel,12,kernel_size=(3,3))  #256\n",
    "        self.down2 = StackEncoder(12,24,kernel_size=(3,3))  # 128\n",
    "        self.down3 = StackEncoder(24,46,kernel_size=(3,3))  # 64\n",
    "        self.down4 = StackEncoder(46,64,kernel_size=(3,3))  # 32\n",
    "        self.down5 = StackEncoder(64,128,kernel_size=(3,3))  #16\n",
    "        \n",
    "        self.center = ConvBlock(128,128,kernel_size=(3,3),padding=1) #16\n",
    "        \n",
    "        self.up5 = StackDecoder(128,128,64,kernel_size=(3,3))  #32\n",
    "        self.up4 = StackDecoder(64,64,46,kernel_size=(3,3)) #64\n",
    "        self.up3 = StackDecoder(46,46,24,kernel_size=(3,3))\n",
    "        self.up2 = StackDecoder(24,24,12,kernel_size=(3,3))\n",
    "        self.up1 = StackDecoder(12,12,12,kernel_size=(3,3))\n",
    "        self.conv = Conv2d(12,1,kernel_size=(1,1),bias=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        down1,out = self.down1(x)  \n",
    "        down2,out = self.down2(out)  \n",
    "        down3,out = self.down3(out)\n",
    "        down4,out = self.down4(out)\n",
    "        down5,out = self.down5(out)\n",
    "        \n",
    "        \n",
    "        out = self.center(out)\n",
    "        \n",
    "        up5 = self.up5(out,down5)\n",
    "        up4 = self.up4(up5,down4)\n",
    "        up3 = self.up3(up4,down3)\n",
    "        up2 = self.up2(up3,down2)\n",
    "        up1 = self.up1(up2,down1)\n",
    "        \n",
    "        out = self.conv(up1)\n",
    "\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the model dry run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((5, 3, 256, 256), dtype=torch.float32)  # minibatch size 64, image size [3, 32, 32]\n",
    "model = Unet256((3,256,256))\n",
    "scores = model(x)\n",
    "print(scores.size())  # you should see [5,1,256,256]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a9fecc71be92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# from torchsummary import summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnet256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "model = Unet256((3,256,256)).to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)       \n",
    "        bce_weight = 0.5\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "#         print(\"Inside DiceBCELoss inputs : {} , targets: {}\".format(inputs.shape, targets.shape))\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        loss_final = BCE * bce_weight + dice_loss * (1 - bce_weight)\n",
    "        return loss_final\n",
    "    \n",
    "    \n",
    "\n",
    "class IoU(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(IoU, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #intersection is equivalent to True Positive count\n",
    "        #union is the mutually inclusive area of all labels & predictions \n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection \n",
    "        \n",
    "        IoU = (intersection + smooth)/(union + smooth)\n",
    "                \n",
    "        return IoU * 100\n",
    "\n",
    "    \n",
    "    \n",
    "class DiceScore(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceScore, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_score = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth) \n",
    "        return dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = DiceBCELoss()\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "    start_time = time.time()\n",
    "     \n",
    "\n",
    "    \n",
    "    running_train_loss = []\n",
    "    \n",
    "    for batch in train_loader: \n",
    "            image,mask,_ = batch[0], batch[1], batch[2]\n",
    "            image = image.to(device,dtype=torch.float)\n",
    "            mask = mask.to(device,dtype=torch.float)\n",
    "            \n",
    "            pred_mask = model.forward(image) # forward propogation\n",
    "            \n",
    "#             print(\"pred_mask : {}, mask : {} \".format(pred_mask.shape, mask.shape))\n",
    "            loss = criterion(pred_mask,mask)\n",
    "            optimizer.zero_grad() # setting gradient to zero\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss.append(loss.item())\n",
    "                              \n",
    "\n",
    "    else:           \n",
    "        running_val_loss = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for image,mask in val_loader:\n",
    "                    image = image.to(device,dtype=torch.float)\n",
    "                    mask = mask.to(device,dtype=torch.float)                            \n",
    "                    pred_mask = model.forward(image)\n",
    "                    loss = criterion(pred_mask,mask)\n",
    "                    running_val_loss.append(loss.item())\n",
    "                    \n",
    "\n",
    "                                    \n",
    "    \n",
    "    epoch_train_loss = np.mean(running_train_loss) \n",
    "    print('Train loss: {}'.format(epoch_train_loss))                       \n",
    "    train_loss.append(epoch_train_loss)\n",
    "    \n",
    "    epoch_val_loss = np.mean(running_val_loss)\n",
    "    print('Validation loss: {}'.format(epoch_val_loss))                                \n",
    "    val_loss.append(epoch_val_loss)\n",
    "                      \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss,label='train_loss')\n",
    "plt.plot(val_loss,label='val_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs231n]",
   "language": "python",
   "name": "conda-env-cs231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
