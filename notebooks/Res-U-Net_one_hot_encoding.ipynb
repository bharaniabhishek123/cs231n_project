{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import pandas as pd\n",
    "import os\n",
    "import skimage \n",
    "from skimage import io \n",
    "import skimage.transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from collections import defaultdict\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 6)\n"
     ]
    }
   ],
   "source": [
    "# Remove wrong values in small image (due to interpolation)\n",
    "import cv2\n",
    "\n",
    "def resize(img, height, width,\n",
    "           interpolation=cv2.INTER_NEAREST):\n",
    "    #use interpolation=cv2.INTER_CUBIC for image \n",
    "    \"\"\"\n",
    "    Resize an image using OpenCV.\n",
    "    Note that cv2.resize dsize is (height, width).\n",
    "\n",
    "    :param img: Input image\n",
    "    :param height: height\n",
    "    :param width: width\n",
    "    :param interpolation: cv2 interpolation type\n",
    "    :return: resized image\n",
    "    \"\"\"\n",
    "    return cv2.resize(img, (height, width), interpolation=interpolation)\n",
    "\n",
    "\n",
    "def one_hot_conversion(mask):\n",
    "\n",
    "    \"\"\" Expects input to be in shape h, w, c \n",
    "    \"\"\"\n",
    "    resized_mask = mask[:,:,0]\n",
    "    num_classes = 6\n",
    "    one_hot_correct = np.zeros((resized_mask.shape[0], resized_mask.shape[1], num_classes))\n",
    "\n",
    "    for class_ in np.unique(resized_mask):\n",
    "        one_hot_correct[:, :, class_][resized_mask == class_] = 1\n",
    "#     print(\"one_hot_correct.shape\", one_hot_correct.shape)\n",
    "    return one_hot_correct # h , w , num_classes\n",
    "\n",
    "\n",
    "mask = skimage.io.MultiImage('/project/data/train_label_masks/0018ae58b01bdadc8e347995b69f99aa_mask.tiff')[-1]\n",
    "resized_mask = resize(mask, 224,224)\n",
    "\n",
    "one_hot_correct = one_hot_conversion(resized_mask)\n",
    "print(one_hot_correct.shape)\n",
    "\n",
    "\n",
    "def mask_from_one_hot(one_hot_mask):\n",
    "    \"\"\" Expects input to be in shape h, w, c\n",
    "    \"\"\"\n",
    "    original_mask = np.argmax(one_hot_mask, axis=2)\n",
    "    return original_mask\n",
    "\n",
    "recovered_mask = mask_from_one_hot(one_hot_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageWithMaskDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_filePath, image_dir, mask_dir, one_hot_mask=False, transform=None):\n",
    "        train_df = pd.read_csv(csv_file_path)\n",
    "                \n",
    "        \n",
    "#         masks = os.listdir(mask_dir)\n",
    "#         masks_df = pd.Series(masks).to_frame()\n",
    "#         masks_df.columns = ['mask_file_name']\n",
    "#         masks_df['image_id'] = masks_df.mask_file_name.apply(lambda x: x.split('_')[0])\n",
    "#         train_df = pd.merge(train_df, masks_df, on='image_id', how='outer')\n",
    "#         del masks_df\n",
    "#         print(f\"There are {len(train_df[train_df.mask_file_name.isna()])} images without a mask.\")\n",
    "\n",
    "#         ## removing items where image mask is null\n",
    "#         train_df = train_df[~train_df.mask_file_name.isna()]        \n",
    "        \n",
    "#         images = os.listdir(image_dir)\n",
    "#         images_df = pd.Series(images).to_frame()\n",
    "#         images_df.columns = ['image_file_name']\n",
    "#         images_df['image_id'] = images_df.image_file_name.apply(lambda x: x.split('.')[0])\n",
    "#         train_df = pd.merge(train_df, images_df, on='image_id', how='outer')\n",
    "#         del images_df \n",
    "#         print(f\"There are {len(train_df[train_df.image_file_name.isna()])} image_ids without a image.\")\n",
    "        \n",
    "#         ## Remove image_id not present in image_dir\n",
    "#         train_df = train_df[~train_df.image_file_name.isna()]                \n",
    "\n",
    "        ## Remove image_id not for Karolinska\n",
    "        train_df = train_df[train_df.data_provider=='radboud']\n",
    "\n",
    "        self.data_frame = train_df.copy()\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.one_hot_mask = one_hot_mask\n",
    "        print('Creating dataset with {} examples'.format(len(self.data_frame)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_id = self.data_frame.iloc[idx]['image_id']\n",
    "        image_file_path = os.path.join(self.image_dir, image_id + \".tiff\")\n",
    "        image = skimage.io.MultiImage(image_file_path)[-1]\n",
    "        image = resize(image, 224,224,interpolation=cv2.INTER_CUBIC)                \n",
    "        image = image.transpose((2, 0, 1))  \n",
    "        \n",
    "        try:\n",
    "            mask_file_path = os.path.join(self.mask_dir, image_id + \"_mask.tiff\")\n",
    "            mask = skimage.io.MultiImage(mask_file_path)[-1] # h, w, c\n",
    "            mask = resize(mask, 224,224)\n",
    "            if self.one_hot_mask:\n",
    "                mask = one_hot_conversion(mask)\n",
    "                mask = mask.transpose((2, 0, 1))        \n",
    "            else:\n",
    "                mask = mask.transpose((2, 0, 1))        \n",
    "                mask = mask[0,:,:]\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        isup_grade = self.data_frame.iloc[idx]['isup_grade']\n",
    "        \n",
    "        return (image, mask, isup_grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 5060 examples\n",
      "0 (3, 224, 224) (224, 224)\n"
     ]
    }
   ],
   "source": [
    "image_dir = '/project/data/train_images'\n",
    "mask_dir = '/project/data/train_label_masks'\n",
    "csv_file_path = '/home/abharani/data/train_filtered.csv'\n",
    "\n",
    "dataset_with_mask = ImageWithMaskDataset(csv_file_path, image_dir, mask_dir,one_hot_mask=False)\n",
    "\n",
    "for i in range(len(dataset_with_mask)):\n",
    "    sample = dataset_with_mask[i]\n",
    "    print(i, sample[0].shape, sample[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('/home/abharani/data/train_filtered.csv')\n",
    "# train_df[train_df['data_provider']=='radboud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert mask into pixel wise one hot encoding \n",
    "def pre_process_mask(mask):\n",
    "    pass\n",
    "    \n",
    "    # first channel 0 - 1, pixel wise one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Test, Train and Valid Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset set size 5060\n",
      "Train set size 3415\n",
      "Validation set size 1139\n",
      "Test set size 506\n"
     ]
    }
   ],
   "source": [
    "def train_val_dataset(dataset, val_split=0.25, generate_small=False):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets_created = {}\n",
    "    if generate_small:\n",
    "        print(\"Generating Small Train, Test Dataset\")\n",
    "        datasets_created['train'] = Subset(dataset, train_idx[0:200])\n",
    "        datasets_created['test'] = Subset(dataset, val_idx[0:50])\n",
    "    else:\n",
    "        datasets_created['train'] = Subset(dataset, train_idx)\n",
    "        datasets_created['test'] = Subset(dataset, val_idx)        \n",
    "    return datasets_created\n",
    "\n",
    "\n",
    "\n",
    "dataset_final = {}\n",
    "datasets_created_trial_1 = train_val_dataset(dataset_with_mask,val_split=0.10,generate_small=False)\n",
    "datasets_created_trial_2 = train_val_dataset(datasets_created_trial_1['train'],val_split=0.25,generate_small=False)\n",
    "\n",
    "dataset_final['test'] = datasets_created_trial_1['test']\n",
    "dataset_final['train'] = datasets_created_trial_2['train']\n",
    "dataset_final['val'] = datasets_created_trial_2['test']\n",
    "\n",
    "print(\"Dataset set size {}\".format(len(dataset_with_mask)))\n",
    "print(\"Train set size {}\".format(len(dataset_final['train'])))\n",
    "print(\"Validation set size {}\".format(len(dataset_final['val'])))\n",
    "print(\"Test set size {}\".format(len(dataset_final['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, sample in enumerate(dataset_final['train']): \n",
    "#     print(sample[0].shape, sample[1].shape)\n",
    "#     if i==2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict \n",
    "\n",
    "# count_dict = defaultdict(int)\n",
    "# target_list = []\n",
    "\n",
    "# #Generate target_list of all labels and count dict of all classes\n",
    "\n",
    "# for i, sample in enumerate(dataset_final['train']): \n",
    "#     label = sample[2]\n",
    "#     count_dict[label] += 1\n",
    "#     target_list.append(int(label))\n",
    "    \n",
    "# print(\"Distribution of classes: \\n\", count_dict)\n",
    "\n",
    "# class_count = [i for i in count_dict.values()]\n",
    "# class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "# print(\"Class weights : \\n\", class_weights)\n",
    "\n",
    "# target_list = torch.tensor(target_list)\n",
    "# target_list = target_list[torch.randperm(len(target_list))]\n",
    "# class_weights_all = class_weights[target_list]\n",
    "\n",
    "# weighted_sampler = WeightedRandomSampler(\n",
    "#     weights=class_weights_all,\n",
    "#     num_samples=len(class_weights_all),\n",
    "#     replacement=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_final['train'],batch_size=32, collate_fn=collate_fn, shuffle = True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(dataset_final['val'],batch_size=32,  collate_fn=collate_fn,num_workers=4, pin_memory=True)\n",
    "\n",
    "dataloaders  = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "dataset_sizes = {x: len(dataset_final[x]) for x in dataset_final.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = None\n",
    "# for i, batch in enumerate(train_loader):\n",
    "    \n",
    "#     image, mask, isup_grade = batch[0], batch[1], batch[2]\n",
    "#     print(i, image.shape, mask.shape)\n",
    " \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.base_layers = list(base_model.children())                \n",
    "        \n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)  \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        self.conv_ultimate = nn.Conv2d(n_class,1,kernel_size=(1,1),bias=True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(input)            \n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)       # (N,6,224,224) \n",
    "        \n",
    "        out_ultimate = self.conv_ultimate(out)\n",
    "        return out_ultimate\n",
    "#         return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 224, 224])\n",
      "torch.Size([5, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((5, 3, 224, 224), dtype=torch.float32)  # minibatch size 64, image size [3, 32, 32]\n",
    "base_model = models.resnet18(pretrained=False)\n",
    "model = ResNetUNet(6)\n",
    "scores = model(x)\n",
    "print(scores.size())  # you should see [5,1,224,224]\n",
    "scores = torch.squeeze(scores)\n",
    "print(scores.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = models.resnet18(pretrained=False)\n",
    "base_model = base_model.to(device)\n",
    "model = ResNetUNet(6)\n",
    "model = model.to(device,dtype = torch.float32)\n",
    "\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        \n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                \n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for batch in dataloaders[phase]:\n",
    "                inputs, labels, label = batch[0],batch[1],batch[2] \n",
    "                inputs = inputs.to(device, dtype = torch.float32)\n",
    "#                 inputs = torch.reshape(inputs,(inputs.shape[0],inputs.shape[3],inputs.shape[1],inputs.shape[2])) #  inputs.reshape [N, C, W, H]\n",
    "#                 print(inputs.shape)\n",
    "                \n",
    "                labels = labels.to(device, dtype = torch.float32)    \n",
    "#                 labels = torch.reshape(labels,(labels.shape[0],labels.shape[3],labels.shape[1],labels.shape[2])) #  inputs.reshape [N, C, W, H]\n",
    "#                 print(labels.shape)\n",
    "\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "#                     outputs = torch.squeeze(outputs) # added now\n",
    "#                     print(\"outputs shape \", outputs.shape, \"labels shape \", labels.shape)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            \n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/14\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 784.00 MiB (GPU 0; 11.17 GiB total capacity; 10.23 GiB already allocated; 247.69 MiB free; 10.61 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-4a0358be160a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-e060a57e50d6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# added now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputs shape \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"labels shape \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-aa91d7a217b7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_up0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_original\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_original_size2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/torch/nn/modules/upsampling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cs231n/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners)\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got 4D input, but linear mode needs 3D input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_bilinear2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_output_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trilinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got 4D input, but trilinear mode needs 5D input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 784.00 MiB (GPU 0; 11.17 GiB total capacity; 10.23 GiB already allocated; 247.69 MiB free; 10.61 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 6\n",
    "\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "# Comment out to finetune further\n",
    "for l in model.base_layers:\n",
    "    for param in l.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)        \n",
    "        \n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/home/abharani/data/ResUNet_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TheModelClass(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = TheModelClass(*args, **kwargs)\n",
    "model = ResNetUNet(6).to(device)\n",
    "# model= MyModel()\n",
    "model.load_state_dict(torch.load('/home/abharani/data/ResUNet_v1'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tensor to image\n",
    "def image_convert(image):\n",
    "    image = image.clone().cpu().numpy()\n",
    "    image = image.transpose((1,2,0))\n",
    "    image = (image * 255)\n",
    "    return image\n",
    "\n",
    "def mask_convert(mask):\n",
    "    mask = mask.clone().cpu().detach().numpy()\n",
    "    return np.squeeze(mask)\n",
    "\n",
    "def plot_img(no_):\n",
    "    iter_ = iter(train_loader)\n",
    "    images,masks, _ = next(iter_)\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for idx in range(0,no_):\n",
    "         image = image_convert(images[idx])\n",
    "         plt.subplot(2,no_,idx+1)\n",
    "         plt.imshow(image)\n",
    "    for idx in range(0,no_):\n",
    "         mask = mask_convert(masks[idx])\n",
    "         plt.subplot(2,no_,idx+no_+1)\n",
    "        \n",
    "         plt.imshow(mask,cmap=cmap, interpolation='nearest',)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#### prediction\n",
    "\n",
    "import math\n",
    "\n",
    "model.eval()   # Set model to evaluate mode\n",
    "\n",
    "test_loader = DataLoader(dataset_final['test'],batch_size=32,  collate_fn=collate_fn,num_workers=4, pin_memory=True)\n",
    "\n",
    "t_images, t_masks, isup_grade  = next(iter(test_loader))\n",
    "t_images = t_images.to(device, dtype = torch.float32)\n",
    "t_masks = t_masks.to(device, dtype = torch.float32)\n",
    "# t_images = torch.reshape(t_images,(t_images.shape[0],t_images.shape[3],t_images.shape[1],t_images.shape[2])) #  inputs.reshape [N, C, W, H]\n",
    "# t_masks = torch.reshape(t_masks,(t_masks.shape[0],t_masks.shape[3],t_masks.shape[1],t_masks.shape[2])) #  inputs.reshape [N, C, W, H]\n",
    "\n",
    "\n",
    "t_preds = model(t_images)\n",
    "# t_preds = torch.sigmoid(t_preds)\n",
    "# pred = pred.data.cpu().numpy()\n",
    "print(t_preds.shape)\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "# input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# # Map each channel (i.e. class) to each color\n",
    "# target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "# pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "\n",
    "# helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_preds = t_preds.to(device)\n",
    "t_masks = t_masks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isup_grade[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224]) torch.Size([32, 6, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(t_preds.shape, t_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([224, 224]) torch.Size([224, 224])\n"
     ]
    }
   ],
   "source": [
    "predicted_mask_0 = torch.argmax(t_preds[0], dim=0)\n",
    "original_mask_0 = torch.argmax(t_masks[0], dim=0)\n",
    "print(predicted_mask_0.shape, original_mask_0.shape)\n",
    "\n",
    "# t_pred = mask_convert(t_preds[0])\n",
    "# t_pred = np.argmax(t_pred, axis=2)\n",
    "# plt.figure(figsize=(20,20))\n",
    "# plt.imshow(t_pred)\n",
    "# t_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(original_mask_0.clone().cpu().numpy(),cmap=cmap, interpolation='nearest')\n",
    "plt.show()\n",
    "plt.imshow(predicted_mask_0.clone().cpu().numpy(),cmap=cmap, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.title(\"Predict Mask\")\n",
    "no_ = 1\n",
    "for idx in range(0,no_):\n",
    "#      t_pred = mask_convert(t_preds[idx])\n",
    "     t_pred = mask_convert(predicted_mask)\n",
    "     plt.subplot(2,no_,idx+no_+1)\n",
    "     plt.imshow(t_pred,cmap=cmap, interpolation='nearest',)\n",
    "\n",
    "plt.show()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs231n]",
   "language": "python",
   "name": "conda-env-cs231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
